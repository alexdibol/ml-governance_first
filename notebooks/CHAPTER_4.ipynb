{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","toc_visible":true,"authorship_tag":"ABX9TyNXQys24dRIYNMuDFAnzc4o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#**CHAPTER 4. GRAPH BASED MODELS**\n","---"],"metadata":{"id":"rgzq7ad4oQsb"}},{"cell_type":"markdown","source":["##0.REFERENCE"],"metadata":{"id":"nBRtjho5h2Cg"}},{"cell_type":"markdown","source":["https://claude.ai/share/f79e007e-9471-4e6b-a465-adc17a3a020c"],"metadata":{"id":"nq-CPUZfxpZX"}},{"cell_type":"markdown","source":["##1.CONTEXT"],"metadata":{"id":"qWHgDkx8h5mE"}},{"cell_type":"markdown","source":["**Chapter 4 — Graph-Based Models: Relational Inference**\n","\n","**Where We Are in the Series**\n","\n","The first three chapters built governance discipline layer by layer. Chapter 1 enforced the distinction between patterns and facts in unsupervised clustering through schema validation, stability testing, and abstention gates. Chapter 2 scaled those controls to match the increased capability of nonlinear supervised models, where optimization pressure introduced leakage, proxy exploitation, and temporal decay. Chapter 3 coupled two generative architectures into a single governed system, treating the latent space as an interface artifact requiring its own validation protocol.\n","\n","Each chapter followed the same thesis: **Capability ↑ ⇒ Risk ↑ ⇒ Controls must ↑.** Each introduced a risk category the previous controls were not designed to catch, and each extended the artifact bundle and decision gate accordingly.\n","\n","Chapter 4 introduces **relational risk.** When data is structured as a graph — entities connected by typed edges — the model does not learn from rows in isolation. It learns from neighborhoods, from paths, from the topology of connection itself. That shift from instance-level to relational learning changes what the model can detect, and what can go wrong.\n","\n","**The Objective of This Notebook**\n","\n","The notebook has three learning objectives, each designed to extend the governance competencies built in earlier chapters into a relational setting.\n","\n","The first objective is to understand why relationships become signals. In tabular settings, each row stands alone. A customer's behavior is a function of that customer's features. In a graph, a customer's behavior is also a function of who that customer is connected to, how often, through which channels, and how that neighborhood itself is connected. Adjacency becomes a feature. This is the fundamental insight that makes graph models powerful — and the fundamental reason they require governance controls that tabular models do not. This notebook makes that conversion step visible and auditable. You will see exactly how an adjacency matrix becomes an embedding space (Capsule A) and how it becomes a message-passing substrate (Capsule B), and you will see the governance artifacts that document each transformation.\n","\n","The second objective is to identify where relational risk emerges. Three specific risks are demonstrated, each tied to a structural property of the synthetic graph. **Propagation risk** arises because information — and bias — travels along edges. A GNN with two layers does not just look at a node's immediate neighbors; it aggregates signals from two hops away. If a biased signal enters the graph at any point within that radius, it propagates outward without any single edge being individually suspicious. **Boundary risk** arises because every graph is a subset of a larger possible graph. The edges that are present define what the model can see. The edges that are absent — whether because they were never observed, fell outside a time window, or were excluded by a governance rule — are invisible to the model but may carry information. The notebook tests what happens when the boundary shifts. **Proxy risk** arises because some edges carry ambiguous meaning. A \"shared device\" edge tells you two accounts accessed the same hardware. It does not tell you whether that access was coordinated, coincidental, or the result of a household sharing a single device. The notebook forces this ambiguity into the open by including a shared\\_device edge type and then deliberately excluding it from the GNN adjacency while leaving it in the embedding input, demonstrating how the same edge can be legitimate in one context and dangerous in another.\n","\n","The third objective is to see how governance artifacts constrain interpretation in a relational setting. The artifact bundle produced by this notebook is the largest in the series. It includes everything from previous chapters — schemas, validation logs, split manifests, metrics, stability reports, guardrails, decision gates, model cards, governance memos — plus artifacts that are specific to graph models: a graph construction manifest that documents which node and edge types are in scope and why, a propagation constraints file that records the maximum hop depth and the edge types permitted in message passing, and a sensitivity report that measures how embeddings and predictions shift under structural perturbation.\n","\n","**The Examples Implemented**\n","\n","The notebook builds a single synthetic typed graph and runs two governed capsules against it. The graph contains three node types — accounts, vendors, and devices — and three edge types — transaction (account to vendor), shared\\_device (account to device), and co\\_supply (vendor to vendor). Two nodes are deliberately constructed to demonstrate specific governance scenarios. A hub account is given an artificially high number of transaction edges, creating a spurious centrality signal. A connector account is given an artificially high number of shared\\_device edges, creating a proxy dominance signal. Neither node is labeled as flagged. The point is not that they are suspicious. The point is that their structural position makes them visible to the model, and visibility is not the same as importance.\n","\n","**Capsule A** implements graph embeddings through adjacency matrix factorization via truncated SVD. This is a deliberately simple method — no random walks, no iterative optimization beyond what scikit-learn provides internally. The simplicity is a feature, not a limitation. It isolates the governance questions from the modeling questions. You can understand exactly what the SVD is doing to the adjacency matrix, which means you can reason about exactly what the resulting embeddings represent and what they do not. The capsule then runs three stability tests: edge dropout removes a fraction of edges at random and measures whether the nearest-neighbor structure of the embeddings holds; edge noise adds spurious edges and measures the same; time-window shift excludes edges older than a cutoff and measures whether the embeddings change. Each perturbation produces a neighbor overlap score and a cosine drift score. If either metric breaches its threshold across any perturbation, interpretation is blocked for that capsule.\n","\n","**Capsule B** implements a two-layer graph convolutional network manually in PyTorch, without any graph neural network library. The adjacency matrix is constructed from only the permitted edge types — transaction and co\\_supply — and the shared\\_device edges are excluded by governance rule, not by modeling convenience. The exclusion is documented in the propagation constraints artifact with an explicit justification. Node features include two synthetic continuous features and one intentionally proxy-like feature (a synthetic device count) that is flagged for human review. The GNN trains on a synthetic node classification task — predicting a review\\_flag label that has been assigned to roughly eight percent of accounts at random. The label is teaching-only and is treated as unverified throughout. Sensitivity tests repeat the evaluation under edge dropout and time-window shift, measuring prediction flip rate and score drift. If either metric breaches its threshold, interpretation is blocked for the GNN.\n","\n","**Expected Behavior and Results**\n","\n","When you execute this notebook end to end, several things will happen in a predictable sequence. Validation will pass. The synthetic data generator is deterministic and schema-consistent by construction, so the fail-closed validation gate in Cell 5 will not trigger. If it does, something has gone wrong with the environment or the seed state, and the notebook will halt and write a blocked decision before raising an error.\n","\n","Both capsules will produce embeddings and predictions. The embedding PCA plot will show three visible clusters corresponding to the three node types, because the adjacency structure separates them. The hub node and the connector node will be marked on that plot. Their positions will not necessarily be outliers — their structural anomaly is in degree, not necessarily in embedding location — but they will be visible for inspection. The GNN training loss will decrease over sixty epochs. The validation accuracy will fluctuate, particularly given the class imbalance (roughly eight percent positive labels), and should not be interpreted as evidence of model quality. The confusion matrix will likely show that the model predicts the majority class most of the time. This is expected. The synthetic label has no real signal; the model is fitting noise in the neighborhood structure.\n","\n","The stability tests are where the governance logic becomes consequential. The decision gate in Cell 8 evaluates whether either capsule's sensitivity metrics breached their thresholds. If they did, the overall status is set to **\"abstain,\"** meaning interpretation is not permitted without human review. If both capsules passed, the status is **\"pass\\_exploratory\"** — but the required\\_human\\_review flag remains unconditionally true, and every output carries **\"Not verified.\"**\n","\n","Cell 10 will produce a sample output that includes a deliberately prohibited query — \"Who should we investigate?\" — and the notebook's refusal to answer it. This refusal is not a failure. It is the expected behavior of a governed system encountering a request outside its permitted boundary, and it is recorded as an enforcement event in the guardrails report.\n","\n","The artifact bundle will contain eighteen files, written regardless of whether the decision status is pass, abstain, or blocked. The governance memo separates what is known from what is assumed, what is open from what has been decided. The model card states that **structure is not causality** and that **connectivity is not explanation.** These are not decorative disclaimers. They define the interpretive boundary of everything this notebook produces.\n","\n","Read the governance memo before you read the plots. That is the order this series has been designed for."],"metadata":{"id":"pF_yDrZ7h6_y"}},{"cell_type":"markdown","source":["##2.LIBRARIES AND ENVIRONMENT"],"metadata":{"id":"yeIrVzqFh7fZ"}},{"cell_type":"code","source":["# =============================================================================\n","# CELL 2 — Install + Imports + Global Config + Helpers\n","# =============================================================================\n","\n","# ---------- installs (only what Colab may lack) ----------\n","import subprocess, sys\n","subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"networkx\"])\n","# torch, scikit-learn, matplotlib, numpy, pandas are pre-installed in Colab\n","\n","# ---------- standard imports ----------\n","import os, json, hashlib, time, platform, random, copy\n","from datetime import datetime, timezone, timedelta\n","import numpy as np\n","import pandas as pd\n","import networkx as nx\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, confusion_matrix\n","import matplotlib\n","matplotlib.use(\"Agg\")          # non-interactive backend (safe in Colab pipelines)\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from scipy import sparse as sp\n","\n","# ---------- GLOBAL_CONFIG ----------\n","GLOBAL_CONFIG = {\n","    # ── reproducibility ──\n","    \"seed\": 42,\n","\n","    # ── graph sizes ──\n","    \"n_accounts\": 800,\n","    \"n_vendors\": 250,\n","    \"n_devices\": 200,\n","\n","    # ── edge generation params ──\n","    \"edge_params\": {\n","        \"account_vendor_p\": 0.008,\n","        \"account_device_p\": 0.006,\n","        \"vendor_vendor_p\": 0.004,\n","        \"weight_mean\": 5.0,\n","        \"weight_std\": 3.0,\n","        \"hub_account_id\": 5,\n","        \"hub_extra_edges\": 60,\n","        \"ambiguous_connector_id\": 12,\n","        \"connector_extra_device_edges\": 25\n","    },\n","\n","    # ── time window ──\n","    \"time_window_params\": {\n","        \"start_days_ago\": 180,\n","        \"end_days_ago\": 0\n","    },\n","\n","    # ── Capsule A: Embeddings ──\n","    \"embedding_dim\": 32,\n","    \"svd_n_components\": 32,\n","    \"stability_edge_dropout_p\": 0.12,\n","    \"stability_noise_edges\": 30,\n","    \"stability_time_cutoff_days\": 90,\n","    \"anchor_nodes_k\": 8,\n","    \"neighbor_k\": 10,\n","\n","    # ── Capsule B: GNN ──\n","    \"gnn_hidden_dim\": 64,\n","    \"gnn_epochs\": 60,\n","    \"gnn_lr\": 0.01,\n","    \"gnn_weight_decay\": 5e-4,\n","    \"gnn_max_hops\": 2,\n","    \"gnn_allowed_edge_types\": [\"transaction\", \"co_supply\"],\n","    \"gnn_influence_clip\": 1.0,\n","    \"review_flag_rate\": 0.08,\n","\n","    # ── Stability thresholds ──\n","    \"neighbor_overlap_min\": 0.45,\n","    \"score_drift_max\": 0.35,\n","    \"sensitivity_fail_policy\": \"abstain\"\n","}\n","\n","# ---------- set ALL seeds deterministically ----------\n","def set_all_seeds(seed: int):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","set_all_seeds(GLOBAL_CONFIG[\"seed\"])\n","\n","# ---------- helper functions ----------\n","def stable_json_dump(path: str, obj):\n","    \"\"\"Write JSON with sorted keys, pretty-print, UTF-8.\"\"\"\n","    d = os.path.dirname(path)\n","    if d:\n","        os.makedirs(d, exist_ok=True)\n","    with open(path, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(obj, f, indent=2, sort_keys=True, default=str)\n","\n","def sha256_dict(obj) -> str:\n","    canonical = json.dumps(obj, sort_keys=True, default=str).encode(\"utf-8\")\n","    return hashlib.sha256(canonical).hexdigest()\n","\n","def sha256_file(path: str) -> str:\n","    h = hashlib.sha256()\n","    with open(path, \"rb\") as f:\n","        for chunk in iter(lambda: f.read(8192), b\"\"):\n","            h.update(chunk)\n","    return h.hexdigest()\n","\n","def now_utc_iso() -> str:\n","    return datetime.now(timezone.utc).isoformat()\n","\n","def safe_mkdirs(path: str):\n","    os.makedirs(path, exist_ok=True)\n","\n","def summarize_graph_stats(G) -> dict:\n","    node_types = {}\n","    for n, d in G.nodes(data=True):\n","        t = d.get(\"node_type\", \"unknown\")\n","        node_types[t] = node_types.get(t, 0) + 1\n","    edge_types = {}\n","    for u, v, d in G.edges(data=True):\n","        t = d.get(\"edge_type\", \"unknown\")\n","        edge_types[t] = edge_types.get(t, 0) + 1\n","    return {\n","        \"n_nodes\": G.number_of_nodes(),\n","        \"n_edges\": G.number_of_edges(),\n","        \"node_type_counts\": node_types,\n","        \"edge_type_counts\": edge_types,\n","        \"is_directed\": G.is_directed(),\n","        \"density\": round(nx.density(G), 6)\n","    }\n","\n","print(\"✅ Cell 2 complete — imports, config, and helpers loaded.\")\n","print(f\"   Seed = {GLOBAL_CONFIG['seed']} | Graph target: \"\n","      f\"{GLOBAL_CONFIG['n_accounts']} accounts, \"\n","      f\"{GLOBAL_CONFIG['n_vendors']} vendors, \"\n","      f\"{GLOBAL_CONFIG['n_devices']} devices\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UJQWI5WylEaD","executionInfo":{"status":"ok","timestamp":1769955693440,"user_tz":360,"elapsed":9820,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"b8e072b5-d80c-484a-d5d7-f79ea395f2da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Cell 2 complete — imports, config, and helpers loaded.\n","   Seed = 42 | Graph target: 800 accounts, 250 vendors, 200 devices\n"]}]},{"cell_type":"markdown","source":["##3.RUN INITIALIZATION, FOLDER LAYOUT AND RUN MANIFEST"],"metadata":{"id":"zn9_gpBPh_yn"}},{"cell_type":"markdown","source":["###3.1.OVERVIEW"],"metadata":{"id":"6Sg0lGxQiBj6"}},{"cell_type":"markdown","source":["**Cell 3 — Run Initialization + Folder Layout + Run Manifest**\n","\n","This cell is the first thing that executes after imports and configuration, and it does something that looks invisible but is foundational to every cell that follows: it builds the scaffolding that makes the entire run auditable. Nothing in this notebook produces output before the audit trail exists. That ordering is a governance decision, not a convenience.\n","\n","The cell begins by constructing a deterministic run identifier. This is not a random string. It is a combination of the current timestamp and a cryptographic hash of the configuration dictionary from Cell 2. That means two runs with identical configurations will differ only in their timestamp. Two runs at the same second with different configurations will differ in their hash. The run_id is the single identifier that ties every artifact produced in this notebook back to the exact parameters and environment that produced it.\n","\n","Next, the cell creates the full directory tree where artifacts will be written: schemas, validation, split, metrics, reports, model, decision, risk, memo, and outputs. These ten subdirectories are not optional. They are created whether or not the notebook succeeds. If the notebook halts in Cell 5 due to a validation failure, the directory structure still exists, and the decision file inside it will say \"blocked.\" The artifact record persists even when the analysis does not.\n","\n","The environment fingerprint captures the exact versions of every library in use: Python, NumPy, pandas, NetworkX, scikit-learn, PyTorch, SciPy, and Matplotlib. This is written to disk, not printed to the console. If someone re-runs this notebook six months later on a different machine, the fingerprint will differ, and a reviewer can compare the two to determine whether environment drift could explain any difference in results.\n","\n","The PATHS registry is a single dictionary that maps every artifact name to its full file path. Every subsequent cell writes artifacts by referencing PATHS rather than constructing paths manually. This is a deliberate centralisation. If the directory structure ever changes, only this cell changes. All downstream cells remain untouched.\n","\n","The cell closes by writing three placeholder files: run_manifest.json, decision.json, and risk_log.json. The decision starts as \"pending.\" The risk log starts empty. Both will be overwritten by later cells as the notebook progresses. The point of writing them here is not their content. It is their existence. At every moment during execution, a valid decision file exists on disk. If the notebook crashes between Cell 5 and Cell 8, the decision file says \"pending\" rather than being absent. Absence would be ambiguous. \"Pending\" is a statement.\n"],"metadata":{"id":"B7g7qJgZiEOw"}},{"cell_type":"markdown","source":["###3.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"SqN4lQyuiE7Y"}},{"cell_type":"code","source":["# =============================================================================\n","# CELL 3 — Run Initialization + Folder Layout + Run Manifest\n","# =============================================================================\n","\n","# ---------- deterministic run_id (NO true randomness) ----------\n","_config_hash_seed = sha256_dict(GLOBAL_CONFIG)[:8]\n","_timestamp_tag   = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%S\")\n","RUN_ID           = f\"run_{_timestamp_tag}_{_config_hash_seed}\"\n","\n","# ---------- base artifact root ----------\n","ARTIFACT_ROOT = os.path.join(\".\", \"artifacts\", RUN_ID)\n","\n","# ---------- create EXACT required directory layout ----------\n","_subdirs = [\"schemas\", \"validation\", \"split\", \"metrics\",\n","            \"reports\", \"model\", \"decision\", \"risk\", \"memo\", \"outputs\"]\n","for sub in _subdirs:\n","    safe_mkdirs(os.path.join(ARTIFACT_ROOT, sub))\n","\n","# ---------- environment fingerprint ----------\n","def _pkg_version(name):\n","    try:\n","        import importlib.metadata\n","        return importlib.metadata.version(name)\n","    except Exception:\n","        return \"unknown\"\n","\n","ENV_FINGERPRINT = {\n","    \"python_version\": sys.version,\n","    \"platform\": platform.platform(),\n","    \"numpy\":      _pkg_version(\"numpy\"),\n","    \"pandas\":     _pkg_version(\"pandas\"),\n","    \"networkx\":   _pkg_version(\"networkx\"),\n","    \"scikit-learn\": _pkg_version(\"scikit-learn\"),\n","    \"torch\":      _pkg_version(\"torch\"),\n","    \"scipy\":      _pkg_version(\"scipy\"),\n","    \"matplotlib\": _pkg_version(\"matplotlib\")\n","}\n","\n","# ---------- config hash (config + env subset) ----------\n","CONFIG_HASH = sha256_dict({\"config\": GLOBAL_CONFIG,\n","                           \"env_python\": ENV_FINGERPRINT[\"python_version\"],\n","                           \"env_platform\": ENV_FINGERPRINT[\"platform\"]})\n","\n","# ---------- artifact path registry (used everywhere downstream) ----------\n","PATHS = {\n","    \"root\":                          ARTIFACT_ROOT,\n","    \"run_manifest\":                  os.path.join(ARTIFACT_ROOT, \"run_manifest.json\"),\n","    \"node_schema\":                   os.path.join(ARTIFACT_ROOT, \"schemas\", \"node_schema.json\"),\n","    \"edge_schema\":                   os.path.join(ARTIFACT_ROOT, \"schemas\", \"edge_schema.json\"),\n","    \"graph_construction_manifest\":   os.path.join(ARTIFACT_ROOT, \"schemas\", \"graph_construction_manifest.json\"),\n","    \"data_validation_log\":           os.path.join(ARTIFACT_ROOT, \"validation\", \"data_validation_log.json\"),\n","    \"split_manifest\":                os.path.join(ARTIFACT_ROOT, \"split\", \"split_manifest.json\"),\n","    \"baseline_metrics\":              os.path.join(ARTIFACT_ROOT, \"metrics\", \"baseline_metrics.json\"),\n","    \"train_metrics\":                 os.path.join(ARTIFACT_ROOT, \"metrics\", \"train_metrics.json\"),\n","    \"eval_metrics\":                  os.path.join(ARTIFACT_ROOT, \"metrics\", \"eval_metrics.json\"),\n","    \"propagation_constraints\":       os.path.join(ARTIFACT_ROOT, \"reports\", \"propagation_constraints.json\"),\n","    \"sensitivity_report\":            os.path.join(ARTIFACT_ROOT, \"reports\", \"sensitivity_report.json\"),\n","    \"guardrails_report\":             os.path.join(ARTIFACT_ROOT, \"reports\", \"guardrails_report.json\"),\n","    \"model_card\":                    os.path.join(ARTIFACT_ROOT, \"model\", \"model_card.json\"),\n","    \"decision\":                      os.path.join(ARTIFACT_ROOT, \"decision\", \"decision.json\"),\n","    \"risk_log\":                      os.path.join(ARTIFACT_ROOT, \"risk\", \"risk_log.json\"),\n","    \"governance_memo\":               os.path.join(ARTIFACT_ROOT, \"memo\", \"governance_memo.json\"),\n","    \"sample_outputs\":                os.path.join(ARTIFACT_ROOT, \"outputs\", \"sample_outputs.json\"),\n","    \"bundle_zip\":                    os.path.join(ARTIFACT_ROOT, \"artifacts_bundle.zip\")\n","}\n","\n","# ---------- write run_manifest.json ----------\n","RUN_MANIFEST = {\n","    \"run_id\": RUN_ID,\n","    \"timestamp\": now_utc_iso(),\n","    \"config\": GLOBAL_CONFIG,\n","    \"config_hash\": CONFIG_HASH,\n","    \"env_fingerprint\": ENV_FINGERPRINT,\n","    \"artifact_paths\": {k: v for k, v in PATHS.items() if k != \"root\"}\n","}\n","stable_json_dump(PATHS[\"run_manifest\"], RUN_MANIFEST)\n","\n","# ---------- initialize decision.json placeholder ----------\n","DECISION_PLACEHOLDER = {\n","    \"run_id\": RUN_ID,\n","    \"overall_status\": \"pending\",\n","    \"interpretation_allowed_embeddings\": None,\n","    \"interpretation_allowed_gnn\": None,\n","    \"required_human_review\": True,\n","    \"verification_status\": \"Not verified\",\n","    \"notes\": \"Placeholder — will be finalised in Cell 8.\"\n","}\n","stable_json_dump(PATHS[\"decision\"], DECISION_PLACEHOLDER)\n","\n","# ---------- initialize risk_log.json placeholder ----------\n","RISK_LOG = {\n","    \"run_id\": RUN_ID,\n","    \"verification_status\": \"Not verified\",\n","    \"events\": []   # capsules will append here\n","}\n","stable_json_dump(PATHS[\"risk_log\"], RISK_LOG)\n","\n","# ---------- in-memory mutable stores (capsules mutate these) ----------\n","RISK_LOG_EVENTS   = []          # list; capsules append dicts\n","SENSITIVITY_DATA  = {}          # key: \"embeddings\" / \"gnn\"\n","GUARDRAILS_EVENTS = []          # enforcement events\n","TRAIN_METRICS     = {}\n","EVAL_METRICS      = {}\n","\n","print(\"✅ Cell 3 complete — run initialised.\")\n","print(f\"   RUN_ID        = {RUN_ID}\")\n","print(f\"   CONFIG_HASH   = {CONFIG_HASH[:16]}…\")\n","print(f\"   Artifact root = {ARTIFACT_ROOT}\")"],"metadata":{"id":"rGOCADsbjRic","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769955886631,"user_tz":360,"elapsed":163,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"7df514a8-525e-495f-cc25-da7f9a334ac9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Cell 3 complete — run initialised.\n","   RUN_ID        = run_20260201T142447_d6e413a4\n","   CONFIG_HASH   = 6e7c6b6060052e63…\n","   Artifact root = ./artifacts/run_20260201T142447_d6e413a4\n"]}]},{"cell_type":"markdown","source":["##4.SYNTHETIC GRAPH GENERATOR, SCHEMAS AND CONSTRUCTION MANIFEST"],"metadata":{"id":"7CdFvriyiHh-"}},{"cell_type":"markdown","source":["###4.1.OVERVIEW"],"metadata":{"id":"r7AlxPj_iI11"}},{"cell_type":"markdown","source":["**Cell 4 — Synthetic Graph Generator + Schemas + Construction Manifest**\n","\n","This cell builds the graph that the rest of the notebook governs. It is the longest cell in the notebook, and that length is justified because it is doing three things simultaneously: generating data, documenting what that data means, and recording the decisions made about what to include and what to exclude. In a real-world system, these three activities would happen at different times by different people. Here they are shown together so that their relationship is visible.\n","\n","The node generation phase creates three populations. Accounts get two continuous synthetic features and one feature that is deliberately, explicitly designed to be proxy-like: synth_proxy_device_count, drawn from a Poisson distribution. This feature is not a mistake. It is a teaching instrument. It exists to demonstrate what happens when a feature that correlates with connectivity enters a model without domain review. Vendors and devices receive the same continuous features but their proxy count is set to zero, because in the synthetic scenario only accounts interact with devices.\n","\n","The review_flag label is assigned randomly to approximately eight percent of accounts. Two accounts are then explicitly unflagged: the hub node and the connector node. This is a subtle but important decision. If either special node happened to be flagged, the model might learn to associate high degree with the label, confounding the spurious centrality demonstration. By removing the flag from both, the cell ensures that the governance risks it is designed to demonstrate remain structurally clean.\n","\n","The edge generation phase produces three typed edge populations through independent Bernoulli sampling. Each account-vendor pair has a small probability of producing a transaction edge. Each account-device pair has a small probability of producing a shared\\_device edge. Each vendor-vendor pair has a small probability of producing a co\\_supply edge. After random generation completes, two nodes are surgically augmented: the hub receives extra transaction edges, and the connector receives extra shared\\_device edges. These are not random. They are forced structural anomalies placed into the graph to make specific governance points visible.\n","\n","The schema artifacts are where this cell earns its governance value. The node schema documents every field, its type, its allowed range, and its provenance. The edge schema goes further: it includes an edge\\_semantics section that describes the meaning of each edge type, its direction, and critically, its ambiguity. The shared\\_device entry states explicitly that shared device access does not imply shared intent. The graph construction manifest records the scope boundary, the time window, and the missingness policy: edges that are absent are uninformative, not evidence of disconnection. A human reviewer reading these three artifacts can reconstruct exactly what this graph contains and why.\n"],"metadata":{"id":"ueNd8fXviKfV"}},{"cell_type":"markdown","source":["###4.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"WYXG5UW3iK2O"}},{"cell_type":"code","source":["# =============================================================================\n","# CELL 4 — Synthetic Graph Generator + Schemas + Construction Manifest\n","# =============================================================================\n","set_all_seeds(GLOBAL_CONFIG[\"seed\"])   # re-seed for reproducibility\n","\n","cfg  = GLOBAL_CONFIG\n","ep   = cfg[\"edge_params\"]\n","tp   = cfg[\"time_window_params\"]\n","now  = datetime.now(timezone.utc)\n","\n","# ---------- 1. Build node table ----------\n","accounts = pd.DataFrame({\n","    \"node_id\":   [f\"acct_{i:04d}\" for i in range(cfg[\"n_accounts\"])],\n","    \"node_type\": \"account\",\n","    \"synth_feature_1\": np.round(np.random.randn(cfg[\"n_accounts\"]), 4),\n","    \"synth_feature_2\": np.round(np.random.rand(cfg[\"n_accounts\"]) * 10, 4),\n","    # intentionally proxy-like feature (for GNN governance demo)\n","    \"synth_proxy_device_count\": np.random.poisson(2, cfg[\"n_accounts\"])\n","})\n","vendors  = pd.DataFrame({\n","    \"node_id\":   [f\"vend_{i:04d}\" for i in range(cfg[\"n_vendors\"])],\n","    \"node_type\": \"vendor\",\n","    \"synth_feature_1\": np.round(np.random.randn(cfg[\"n_vendors\"]), 4),\n","    \"synth_feature_2\": np.round(np.random.rand(cfg[\"n_vendors\"]) * 10, 4),\n","    \"synth_proxy_device_count\": 0\n","})\n","devices  = pd.DataFrame({\n","    \"node_id\":   [f\"dev_{i:04d}\"  for i in range(cfg[\"n_devices\"])],\n","    \"node_type\": \"device\",\n","    \"synth_feature_1\": np.round(np.random.randn(cfg[\"n_devices\"]), 4),\n","    \"synth_feature_2\": np.round(np.random.rand(cfg[\"n_devices\"]) * 10, 4),\n","    \"synth_proxy_device_count\": 0\n","})\n","node_df = pd.concat([accounts, vendors, devices], ignore_index=True)\n","\n","# ---------- 2. Assign synthetic review_flag (accounts only, sparse) ----------\n","n_flagged = max(1, int(cfg[\"n_accounts\"] * cfg[\"review_flag_rate\"]))\n","flagged_indices = np.random.choice(cfg[\"n_accounts\"], size=n_flagged, replace=False)\n","node_df[\"review_flag\"] = 0\n","node_df.loc[flagged_indices, \"review_flag\"] = 1\n","# make sure hub and connector are NOT automatically flagged (to avoid confounding)\n","hub_idx          = node_df.index[node_df[\"node_id\"] == f\"acct_{ep['hub_account_id']:04d}\"].tolist()\n","connector_idx    = node_df.index[node_df[\"node_id\"] == f\"acct_{ep['ambiguous_connector_id']:04d}\"].tolist()\n","for idx in hub_idx + connector_idx:\n","    node_df.loc[idx, \"review_flag\"] = 0\n","\n","# ---------- 3. Build edge list ----------\n","acct_ids  = node_df.loc[node_df[\"node_type\"] == \"account\", \"node_id\"].tolist()\n","vend_ids  = node_df.loc[node_df[\"node_type\"] == \"vendor\",  \"node_id\"].tolist()\n","dev_ids   = node_df.loc[node_df[\"node_type\"] == \"device\",  \"node_id\"].tolist()\n","\n","def _rand_timestamp():\n","    days_back = np.random.randint(tp[\"end_days_ago\"], tp[\"start_days_ago\"])\n","    return (now - timedelta(days=int(days_back))).isoformat()\n","\n","edges = []   # list of dicts\n","\n","# (a) account → vendor  \"transaction\"\n","for a in acct_ids:\n","    for v in vend_ids:\n","        if np.random.rand() < ep[\"account_vendor_p\"]:\n","            w = max(1, round(np.random.normal(ep[\"weight_mean\"], ep[\"weight_std\"]), 2))\n","            edges.append({\"src\": a, \"dst\": v, \"edge_type\": \"transaction\",\n","                          \"weight\": w, \"timestamp\": _rand_timestamp()})\n","\n","# (b) account ↔ device  \"shared_device\"  (ambiguous proxy channel)\n","for a in acct_ids:\n","    for d in dev_ids:\n","        if np.random.rand() < ep[\"account_device_p\"]:\n","            edges.append({\"src\": a, \"dst\": d, \"edge_type\": \"shared_device\",\n","                          \"weight\": 1.0, \"timestamp\": _rand_timestamp()})\n","\n","# (c) vendor ↔ vendor  \"co_supply\"\n","for i, v1 in enumerate(vend_ids):\n","    for v2 in vend_ids[i+1:]:\n","        if np.random.rand() < ep[\"vendor_vendor_p\"]:\n","            edges.append({\"src\": v1, \"dst\": v2, \"edge_type\": \"co_supply\",\n","                          \"weight\": 1.0, \"timestamp\": _rand_timestamp()})\n","\n","# ---------- 4. Force hub node (spurious centrality) ----------\n","hub_id = f\"acct_{ep['hub_account_id']:04d}\"\n","hub_targets = np.random.choice(vend_ids, size=min(ep[\"hub_extra_edges\"], len(vend_ids)), replace=False)\n","for v in hub_targets:\n","    edges.append({\"src\": hub_id, \"dst\": v, \"edge_type\": \"transaction\",\n","                  \"weight\": round(np.random.uniform(1, 3), 2), \"timestamp\": _rand_timestamp()})\n","\n","# ---------- 5. Force ambiguous connector (proxy dominance) ----------\n","conn_id   = f\"acct_{ep['ambiguous_connector_id']:04d}\"\n","conn_devs = np.random.choice(dev_ids, size=min(ep[\"connector_extra_device_edges\"], len(dev_ids)), replace=False)\n","for d in conn_devs:\n","    edges.append({\"src\": conn_id, \"dst\": d, \"edge_type\": \"shared_device\",\n","                  \"weight\": 1.0, \"timestamp\": _rand_timestamp()})\n","\n","edge_df = pd.DataFrame(edges)\n","print(f\"   Generated {len(node_df)} nodes, {len(edge_df)} edges\")\n","\n","# ---------- 6. Build NetworkX MultiDiGraph ----------\n","G = nx.MultiDiGraph()\n","for _, row in node_df.iterrows():\n","    G.add_node(row[\"node_id\"],\n","               node_type=row[\"node_type\"],\n","               synth_feature_1=float(row[\"synth_feature_1\"]),\n","               synth_feature_2=float(row[\"synth_feature_2\"]),\n","               synth_proxy_device_count=int(row[\"synth_proxy_device_count\"]),\n","               review_flag=int(row[\"review_flag\"]))\n","for _, row in edge_df.iterrows():\n","    G.add_edge(row[\"src\"], row[\"dst\"],\n","               edge_type=row[\"edge_type\"],\n","               weight=float(row[\"weight\"]),\n","               timestamp=row[\"timestamp\"])\n","\n","# ---------- 7. Write schemas ----------\n","NODE_SCHEMA = {\n","    \"schema_version\": \"1.0\",\n","    \"source\": \"synthetic_generator\",\n","    \"rationale\": \"Typed node table for governance demonstration with account/vendor/device roles.\",\n","    \"fields\": {\n","        \"node_id\":   {\"type\": \"string\", \"pattern\": \"^(acct|vend|dev)_\\\\d{4}$\", \"required\": True},\n","        \"node_type\": {\"type\": \"string\", \"allowed\": [\"account\", \"vendor\", \"device\"], \"required\": True},\n","        \"synth_feature_1\": {\"type\": \"float\", \"required\": True, \"provenance\": \"synthetic_normal\"},\n","        \"synth_feature_2\": {\"type\": \"float\", \"min\": 0.0, \"max\": 10.0, \"required\": True, \"provenance\": \"synthetic_uniform\"},\n","        \"synth_proxy_device_count\": {\"type\": \"int\", \"min\": 0, \"required\": True,\n","                                     \"provenance\": \"synthetic_poisson\",\n","                                     \"governance_note\": \"Intentional proxy feature — demonstrates why feature provenance review is mandatory.\"},\n","        \"review_flag\": {\"type\": \"int\", \"allowed\": [0, 1], \"required\": True,\n","                        \"provenance\": \"synthetic_assignment\",\n","                        \"governance_note\": \"Teaching label only. Not verified. Must not drive real decisions.\"}\n","    }\n","}\n","stable_json_dump(PATHS[\"node_schema\"], NODE_SCHEMA)\n","\n","EDGE_SCHEMA = {\n","    \"schema_version\": \"1.0\",\n","    \"source\": \"synthetic_generator\",\n","    \"rationale\": \"Typed edge table preserving semantic distinctions for governance audit.\",\n","    \"fields\": {\n","        \"src\":        {\"type\": \"string\", \"required\": True},\n","        \"dst\":        {\"type\": \"string\", \"required\": True},\n","        \"edge_type\":  {\"type\": \"string\", \"allowed\": [\"transaction\", \"shared_device\", \"co_supply\"], \"required\": True},\n","        \"weight\":     {\"type\": \"float\", \"min\": 0.0, \"required\": True},\n","        \"timestamp\":  {\"type\": \"iso8601\", \"required\": True}\n","    },\n","    \"edge_semantics\": {\n","        \"transaction\": {\n","            \"direction\": \"account→vendor\",\n","            \"meaning\": \"Recorded commercial interaction (synthetic frequency).\",\n","            \"ambiguity_notes\": \"Weight is synthetic frequency; does not represent real monetary value.\"\n","        },\n","        \"shared_device\": {\n","            \"direction\": \"account↔device (undirected semantically)\",\n","            \"meaning\": \"Two or more accounts accessed the same device (synthetic).\",\n","            \"ambiguity_notes\": \"HIGH AMBIGUITY — shared device does not imply shared intent, collusion, or fraud. This is the primary proxy-dominance risk channel. Excluded from GNN adjacency by default.\"\n","        },\n","        \"co_supply\": {\n","            \"direction\": \"vendor↔vendor (undirected semantically)\",\n","            \"meaning\": \"Vendors share synthetic supply-chain attributes.\",\n","            \"ambiguity_notes\": \"Similarity edge; does not imply causal or contractual relationship.\"\n","        }\n","    }\n","}\n","stable_json_dump(PATHS[\"edge_schema\"], EDGE_SCHEMA)\n","\n","# ---------- 8. Write graph_construction_manifest ----------\n","GRAPH_CONSTRUCTION_MANIFEST = {\n","    \"source\": \"synthetic_generator\",\n","    \"scope_boundary\": {\n","        \"included_node_types\": [\"account\", \"vendor\", \"device\"],\n","        \"included_edge_types\": [\"transaction\", \"shared_device\", \"co_supply\"],\n","        \"excluded_contexts\": [\n","            \"No real-world data included.\",\n","            \"No external APIs or databases queried.\",\n","            \"No privileged or PII data present.\"\n","        ]\n","    },\n","    \"time_window\": {\n","        \"start\": (now - timedelta(days=tp[\"start_days_ago\"])).isoformat(),\n","        \"end\": now.isoformat(),\n","        \"rule\": \"All edge timestamps fall within [start, end]. Time-window shift tests will move this boundary.\"\n","    },\n","    \"missingness_semantics\": {\n","        \"policy\": \"Edges not present are treated as 'no observed interaction in window' — NOT as 'known absence'.\",\n","        \"implication\": \"Missing edges are uninformative, not evidence of disconnection.\"\n","    },\n","    \"special_nodes\": {\n","        \"hub_node\": {\n","            \"id\": hub_id,\n","            \"role\": \"Forced high-degree account for spurious centrality demonstration.\",\n","            \"governance_note\": \"High degree does NOT imply importance or risk.\"\n","        },\n","        \"ambiguous_connector\": {\n","            \"id\": conn_id,\n","            \"role\": \"Forced high shared_device degree for proxy dominance demonstration.\",\n","            \"governance_note\": \"Many shared devices does NOT imply shared intent.\"\n","        }\n","    },\n","    \"graph_stats\": summarize_graph_stats(G),\n","    \"verification_status\": \"Not verified\"\n","}\n","stable_json_dump(PATHS[\"graph_construction_manifest\"], GRAPH_CONSTRUCTION_MANIFEST)\n","\n","print(\"✅ Cell 4 complete — synthetic graph built and schemas written.\")\n","print(f\"   {summarize_graph_stats(G)}\")"],"metadata":{"id":"zwXf8QYdjSeI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769955929555,"user_tz":360,"elapsed":673,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"dd8bee67-e4f7-4aff-e69d-e1f4ad9725a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   Generated 1250 nodes, 2742 edges\n","✅ Cell 4 complete — synthetic graph built and schemas written.\n","   {'n_nodes': 1250, 'n_edges': 2742, 'node_type_counts': {'account': 800, 'vendor': 250, 'device': 200}, 'edge_type_counts': {'transaction': 1621, 'shared_device': 990, 'co_supply': 131}, 'is_directed': True, 'density': 0.001756}\n"]}]},{"cell_type":"markdown","source":["##5.VALIDATION AND SPLIT MANIFEST. GRAPH VISUALIZATION"],"metadata":{"id":"PJydwmXniOfj"}},{"cell_type":"markdown","source":["###5.1.OVERVIEW"],"metadata":{"id":"MeZ0d501iP49"}},{"cell_type":"markdown","source":["**Cell 5 — Validation + Split Manifest + Graph Visualization**\n","\n","This cell is the gatekeeper. Everything before it generates data and documents it. Everything after it models that data and interprets it. This cell sits between those two phases and asks a single question: is the data what it claims to be? If the answer is no, the notebook stops. Not with a warning. Not with a suggestion to proceed with caution. It stops completely, writes \"blocked\" to the decision file, and raises a SystemExit. This is fail-closed logic, and it is the first place in the notebook where the governance framework exercises real authority.\n","\n","The validation runs twelve rules across three categories. Node validations check that all node types are in the allowed set, that all required fields are present, that node identifiers are unique, that synth\\_feature\\_2 falls within its declared range of zero to ten, that review\\_flag contains only zero or one, and that the proxy feature is non-negative. Edge validations check that edge types are in the allowed set, that required fields are present, that all weights are positive, that every source and destination node actually exists in the node table, and that edge endpoints are type-consistent: transaction edges must connect accounts to vendors, shared\\_device edges must connect accounts to devices, and co\\_supply edges must connect vendors to vendors. Structural sanity checks flag any node whose degree exceeds five times the median (informational only, not blocking) and confirm that no self-loops exist.\n","\n","Every rule is recorded in the validation log with its name, its pass/fail status, and a detail string. This log is written to disk before the fail-closed check executes. Even if the notebook halts, the log exists and a reviewer can read exactly which rule failed and why.\n","\n","The split manifest creates two independent splits tailored to the two capsules. Capsule A needs an edge holdout for link-prediction evaluation, so fifteen percent of edges are set aside. Capsule B needs a node-level train/validation/test split among accounts only, so the account population is divided sixty-twenty-twenty. Each split is hashed. If someone re-runs the notebook with the same seed, the hashes will match. If a seed changes, the mismatch is detectable.\n","\n","The visualization at the end of this cell draws an induced subgraph: forty randomly sampled accounts plus all of their one-hop neighbors. Nodes are colored by type. The plot is saved as a PNG and labeled \"Not verified.\" Its purpose is not analysis. It is orientation. It shows the student what the graph looks like before any model touches it.\n"],"metadata":{"id":"WY3hc3q2iSKx"}},{"cell_type":"markdown","source":["###5.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"fzeMhbzgiShs"}},{"cell_type":"code","source":["\n","# =============================================================================\n","# CELL 5 — Validation + Split Manifest + Graph Visualization\n","# =============================================================================\n","\n","validation_rules = []   # accumulate rule results\n","\n","def _record(rule_name, passed, detail=\"\"):\n","    validation_rules.append({\n","        \"rule\": rule_name,\n","        \"passed\": passed,\n","        \"detail\": detail\n","    })\n","\n","# ---------- 1. Node validations ----------\n","# 1a. Type check: all node_types in allowed set\n","allowed_node_types = {\"account\", \"vendor\", \"device\"}\n","bad_ntypes = set(node_df[\"node_type\"].unique()) - allowed_node_types\n","_record(\"node_type_allowed\", len(bad_ntypes) == 0,\n","        f\"Unexpected types: {bad_ntypes}\" if bad_ntypes else \"All node types valid.\")\n","\n","# 1b. Required fields present\n","required_node_fields = [\"node_id\", \"node_type\", \"synth_feature_1\", \"synth_feature_2\",\n","                        \"synth_proxy_device_count\", \"review_flag\"]\n","missing_nf = [f for f in required_node_fields if f not in node_df.columns]\n","_record(\"node_required_fields\", len(missing_nf) == 0,\n","        f\"Missing: {missing_nf}\" if missing_nf else \"All required node fields present.\")\n","\n","# 1c. node_id uniqueness\n","_record(\"node_id_unique\", node_df[\"node_id\"].is_unique,\n","        f\"Duplicates: {node_df[node_df['node_id'].duplicated()]['node_id'].tolist()[:5]}\"\n","        if not node_df[\"node_id\"].is_unique else \"All node_ids unique.\")\n","\n","# 1d. synth_feature_2 range [0, 10]\n","f2_ok = ((node_df[\"synth_feature_2\"] >= 0.0) & (node_df[\"synth_feature_2\"] <= 10.0)).all()\n","_record(\"node_feature_2_range\", bool(f2_ok), \"synth_feature_2 in [0,10].\")\n","\n","# 1e. review_flag only 0/1\n","rf_ok = node_df[\"review_flag\"].isin([0, 1]).all()\n","_record(\"node_review_flag_values\", bool(rf_ok), \"review_flag ∈ {0,1}.\")\n","\n","# 1f. proxy feature non-negative\n","pf_ok = (node_df[\"synth_proxy_device_count\"] >= 0).all()\n","_record(\"node_proxy_feature_nonneg\", bool(pf_ok), \"synth_proxy_device_count ≥ 0.\")\n","\n","# ---------- 2. Edge validations ----------\n","allowed_edge_types = {\"transaction\", \"shared_device\", \"co_supply\"}\n","bad_etypes = set(edge_df[\"edge_type\"].unique()) - allowed_edge_types\n","_record(\"edge_type_allowed\", len(bad_etypes) == 0,\n","        f\"Unexpected edge types: {bad_etypes}\" if bad_etypes else \"All edge types valid.\")\n","\n","required_edge_fields = [\"src\", \"dst\", \"edge_type\", \"weight\", \"timestamp\"]\n","missing_ef = [f for f in required_edge_fields if f not in edge_df.columns]\n","_record(\"edge_required_fields\", len(missing_ef) == 0,\n","        f\"Missing: {missing_ef}\" if missing_ef else \"All required edge fields present.\")\n","\n","# 2a. Weight > 0\n","w_ok = (edge_df[\"weight\"] > 0).all()\n","_record(\"edge_weight_positive\", bool(w_ok), \"All edge weights > 0.\")\n","\n","# 2b. Endpoints exist in node table\n","all_node_ids = set(node_df[\"node_id\"])\n","bad_src = set(edge_df[\"src\"]) - all_node_ids\n","bad_dst = set(edge_df[\"dst\"]) - all_node_ids\n","_record(\"edge_endpoints_exist\", len(bad_src) == 0 and len(bad_dst) == 0,\n","        f\"Bad src: {list(bad_src)[:3]}, bad dst: {list(bad_dst)[:3]}\"\n","        if (bad_src or bad_dst) else \"All edge endpoints exist in node table.\")\n","\n","# 2c. Endpoint-type consistency\n","node_type_map = dict(zip(node_df[\"node_id\"], node_df[\"node_type\"]))\n","type_violations = []\n","for _, row in edge_df.iterrows():\n","    st, dt = node_type_map.get(row[\"src\"]), node_type_map.get(row[\"dst\"])\n","    if row[\"edge_type\"] == \"transaction\" and not (st == \"account\" and dt == \"vendor\"):\n","        type_violations.append(f\"{row['src']}→{row['dst']}\")\n","    elif row[\"edge_type\"] == \"shared_device\" and not (\n","            (st == \"account\" and dt == \"device\") or (st == \"device\" and dt == \"account\")):\n","        type_violations.append(f\"{row['src']}→{row['dst']}\")\n","    elif row[\"edge_type\"] == \"co_supply\" and not (st == \"vendor\" and dt == \"vendor\"):\n","        type_violations.append(f\"{row['src']}→{row['dst']}\")\n","_record(\"edge_endpoint_type_consistency\", len(type_violations) == 0,\n","        f\"{len(type_violations)} violations (first 3: {type_violations[:3]})\"\n","        if type_violations else \"All endpoint-type pairs consistent.\")\n","\n","# ---------- 3. Structural sanity ----------\n","# 3a. Degree spike: flag if any node degree > 5× median\n","degrees = dict(G.degree())\n","deg_vals = np.array(list(degrees.values()))\n","median_deg = np.median(deg_vals)\n","spike_threshold = 5 * median_deg if median_deg > 0 else 50\n","spikes = [(n, d) for n, d in degrees.items() if d > spike_threshold]\n","_record(\"structural_degree_spike_check\", True,   # informational, not blocking\n","        f\"{len(spikes)} nodes exceed {spike_threshold:.0f} degree (median={median_deg:.1f}). \"\n","        f\"Examples: {spikes[:3]}. NOTE: hub node is by design.\")\n","\n","# 3b. Self-loop check (allowed in MultiDiGraph but flag if any)\n","self_loops = list(nx.selfloop_edges(G))\n","_record(\"structural_no_self_loops\", len(self_loops) == 0,\n","        f\"{len(self_loops)} self-loops found.\" if self_loops else \"No self-loops.\")\n","\n","# ---------- 4. Write validation log ----------\n","all_passed = all(r[\"passed\"] for r in validation_rules\n","                 # degree spike is informational — exclude from pass/fail\n","                 if r[\"rule\"] != \"structural_degree_spike_check\")\n","\n","VALIDATION_LOG = {\n","    \"run_id\": RUN_ID,\n","    \"timestamp\": now_utc_iso(),\n","    \"overall_passed\": all_passed,\n","    \"rules\": validation_rules,\n","    \"verification_status\": \"Not verified\"\n","}\n","stable_json_dump(PATHS[\"data_validation_log\"], VALIDATION_LOG)\n","\n","# ---------- 5. FAIL-CLOSED: if validation fails, block ----------\n","if not all_passed:\n","    DECISION_PLACEHOLDER[\"overall_status\"] = \"blocked\"\n","    DECISION_PLACEHOLDER[\"notes\"] = \"Schema validation failed — notebook halted.\"\n","    stable_json_dump(PATHS[\"decision\"], DECISION_PLACEHOLDER)\n","    raise SystemExit(\"🚨 VALIDATION FAILED — notebook blocked (fail-closed).\")\n","\n","print(f\"   ✅ Validation passed ({len(validation_rules)} rules checked).\")\n","\n","# ---------- 6. Split manifest ----------\n","# For embeddings: edge holdout for link-pred proxy\n","set_all_seeds(GLOBAL_CONFIG[\"seed\"])\n","edge_indices = np.arange(len(edge_df))\n","np.random.shuffle(edge_indices)\n","n_holdout = max(10, int(len(edge_df) * 0.15))\n","holdout_edge_idx  = edge_indices[:n_holdout].tolist()\n","train_edge_idx    = edge_indices[n_holdout:].tolist()\n","\n","# For GNN: node train/val/test split (accounts with labels only)\n","acct_mask = node_df[\"node_type\"] == \"account\"\n","acct_indices = node_df.index[acct_mask].tolist()\n","np.random.shuffle(acct_indices)\n","n_acct = len(acct_indices)\n","n_train = int(n_acct * 0.6)\n","n_val   = int(n_acct * 0.2)\n","gnn_train_idx = sorted(acct_indices[:n_train])\n","gnn_val_idx   = sorted(acct_indices[n_train:n_train+n_val])\n","gnn_test_idx  = sorted(acct_indices[n_train+n_val:])\n","\n","SPLIT_MANIFEST = {\n","    \"run_id\": RUN_ID,\n","    \"embeddings\": {\n","        \"method\": \"edge_holdout_link_pred_proxy\",\n","        \"train_edge_indices\": train_edge_idx,\n","        \"holdout_edge_indices\": holdout_edge_idx,\n","        \"train_hash\": sha256_dict({\"indices\": train_edge_idx}),\n","        \"holdout_hash\": sha256_dict({\"indices\": holdout_edge_idx})\n","    },\n","    \"gnn\": {\n","        \"method\": \"node_stratified_split_accounts_only\",\n","        \"train_indices\": gnn_train_idx,\n","        \"val_indices\":   gnn_val_idx,\n","        \"test_indices\":  gnn_test_idx,\n","        \"train_hash\": sha256_dict({\"indices\": gnn_train_idx}),\n","        \"val_hash\":   sha256_dict({\"indices\": gnn_val_idx}),\n","        \"test_hash\":  sha256_dict({\"indices\": gnn_test_idx})\n","    },\n","    \"verification_status\": \"Not verified\"\n","}\n","stable_json_dump(PATHS[\"split_manifest\"], SPLIT_MANIFEST)\n","\n","# ---------- 7. VISUALIZATION: induced subgraph ----------\n","set_all_seeds(GLOBAL_CONFIG[\"seed\"])\n","sample_accounts = np.random.choice(acct_ids, size=min(40, len(acct_ids)), replace=False).tolist()\n","# gather 1-hop neighbors\n","subgraph_nodes = set(sample_accounts)\n","for a in sample_accounts:\n","    for _, nbr, _ in G.out_edges(a, data=True):\n","        subgraph_nodes.add(nbr)\n","    for pred, _, _ in G.in_edges(a, data=True):\n","        subgraph_nodes.add(pred)\n","subG = G.subgraph(subgraph_nodes).copy()\n","\n","# Convert to simple undirected for layout\n","simple_sub = nx.Graph()\n","for n, d in subG.nodes(data=True):\n","    simple_sub.add_node(n, **d)\n","for u, v, d in subG.edges(data=True):\n","    simple_sub.add_edge(u, v, **d)\n","\n","color_map = {\"account\": \"#4A90D9\", \"vendor\": \"#E67E22\", \"device\": \"#2ECC71\"}\n","node_colors = [color_map.get(simple_sub.nodes[n].get(\"node_type\", \"unknown\"), \"#999\")\n","               for n in simple_sub.nodes()]\n","\n","fig, ax = plt.subplots(figsize=(11, 8))\n","pos = nx.spring_layout(simple_sub, seed=GLOBAL_CONFIG[\"seed\"], k=2.2)\n","nx.draw_networkx_nodes(simple_sub, pos, node_color=node_colors, node_size=90, ax=ax, alpha=0.85)\n","nx.draw_networkx_edges(simple_sub, pos, ax=ax, alpha=0.25, width=0.7, edge_color=\"#666\")\n","\n","# legend\n","from matplotlib.patches import Patch\n","legend_elements = [Patch(facecolor=c, label=t) for t, c in color_map.items()]\n","ax.legend(handles=legend_elements, loc=\"upper left\", fontsize=9, framealpha=0.9)\n","\n","ax.set_title(\"Induced Subgraph — 40 Sampled Accounts + 1-Hop Neighbors\\n\"\n","             \"(Synthetic data · Not verified)\", fontsize=13, fontweight=\"bold\")\n","ax.axis(\"off\")\n","plt.tight_layout()\n","plt.savefig(os.path.join(ARTIFACT_ROOT, \"graph_subgraph_viz.png\"), dpi=130, bbox_inches=\"tight\")\n","plt.show()\n","print(\"   📊 Graph visualization saved.\")\n","\n","# ---------- 8. Baseline metrics placeholder ----------\n","BASELINE_METRICS = {\n","    \"run_id\": RUN_ID,\n","    \"graph_stats\": summarize_graph_stats(G),\n","    \"label_distribution\": {\n","        \"accounts_total\": int((node_df[\"node_type\"] == \"account\").sum()),\n","        \"review_flag_1\": int(node_df[\"review_flag\"].sum()),\n","        \"review_flag_0\": int((node_df[\"review_flag\"] == 0).sum())\n","    },\n","    \"verification_status\": \"Not verified\"\n","}\n","stable_json_dump(PATHS[\"baseline_metrics\"], BASELINE_METRICS)\n","\n","print(\"✅ Cell 5 complete — validation passed, splits created, subgraph drawn.\")"],"metadata":{"id":"t3ZYvRswjUKQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769955991303,"user_tz":360,"elapsed":445,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"53328e20-bf09-4b6a-ea51-785e2dc3fa7c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   ✅ Validation passed (13 rules checked).\n","   📊 Graph visualization saved.\n","✅ Cell 5 complete — validation passed, splits created, subgraph drawn.\n"]}]},{"cell_type":"markdown","source":["##6.GRAPH EMBEDDINGS"],"metadata":{"id":"3qMrcRA8iVGP"}},{"cell_type":"markdown","source":["###6.1.OVERVIEW"],"metadata":{"id":"3Ak9KwaQiXOz"}},{"cell_type":"markdown","source":["**Cell 6 — Governed Capsule A: Graph Embeddings**\n","\n","This cell executes the first complete governed model pipeline in the notebook. It takes the graph built in Cell 4, converts its structure into dense numerical vectors, evaluates those vectors, stress-tests them, checks whether they are stable enough to interpret, and records every step. The method is deliberately simple: truncated SVD applied directly to the adjacency matrix. No random walks, no iterative optimization beyond what scikit-learn provides internally. The simplicity is the point. When a method is simple enough to trace by hand, governance questions become answerable.\n","\n","The adjacency matrix is built using only the training edges from the split manifest. The holdout edges are invisible to the embedding. SVD compresses the adjacency into a thirty-two-dimensional representation, and each row is L2-normalized so that cosine similarity becomes the natural distance metric. A link-prediction AUC is computed on the holdout edges as a secondary metric, but this is explicitly labeled secondary. It is a proxy check, not a performance claim.\n","\n","The stability tests are where this cell demonstrates what governance actually does to an embedding pipeline. Three perturbations are applied independently. Edge dropout removes a random fraction of training edges and refits the embeddings. Edge noise adds spurious random edges and refits. Time-window shift excludes edges older than a cutoff and refits. For each perturbation, two metrics are computed: neighbor overlap at k, which measures whether the nearest neighbors of anchor nodes remain the same, and cosine drift, which measures how far each anchor's embedding vector has moved. If either metric breaches its threshold across any perturbation, interpretation is blocked for this capsule entirely.\n","\n","The PCA visualization projects the thirty-two-dimensional embeddings into two dimensions. Three clusters appear, corresponding to the three node types. The hub node and connector node are marked with distinct symbols. Their positions are not necessarily outliers. Their structural anomaly lives in degree, not necessarily in embedding location. A student examining this plot must resist the temptation to read meaning into cluster membership. The clusters reflect adjacency structure, not behavior or intent. The stability bar charts show overlap and drift side by side across the three perturbations, with threshold lines drawn in red. A student reading these charts can see exactly whether this capsule passed or failed, and why. If a bar crosses the red line, interpretation is blocked. The chart does not suggest what to do next. It records what happened and leaves the next decision to the human reviewer who reads the governance memo.\n"],"metadata":{"id":"Fnhd7Fwaicyv"}},{"cell_type":"markdown","source":["###6.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"0jFxOakbido7"}},{"cell_type":"code","source":["\n","# =============================================================================\n","# CELL 6 — Governed Capsule A: Graph Embeddings (End-to-End + Visuals)\n","# =============================================================================\n","\n","def _build_adjacency(graph, node_list, edge_indices=None, edge_df_ref=None):\n","    \"\"\"\n","    Build a symmetric adjacency matrix (scipy sparse) from the graph.\n","    If edge_indices + edge_df_ref are given, only those edges are used.\n","    Otherwise all edges in graph are used.\n","    \"\"\"\n","    n = len(node_list)\n","    idx_map = {nid: i for i, nid in enumerate(node_list)}\n","    rows, cols, vals = [], [], []\n","\n","    if edge_indices is not None and edge_df_ref is not None:\n","        for ei in edge_indices:\n","            row = edge_df_ref.iloc[ei]\n","            s, d = row[\"src\"], row[\"dst\"]\n","            if s in idx_map and d in idx_map:\n","                si, di = idx_map[s], idx_map[d]\n","                w = row[\"weight\"]\n","                rows.extend([si, di]); cols.extend([di, si]); vals.extend([w, w])\n","    else:\n","        for u, v, d in graph.edges(data=True):\n","            if u in idx_map and v in idx_map:\n","                si, di = idx_map[u], idx_map[v]\n","                w = d.get(\"weight\", 1.0)\n","                rows.extend([si, di]); cols.extend([di, si]); vals.extend([w, w])\n","\n","    A = sp.csr_matrix((vals, (rows, cols)), shape=(n, n))\n","    return A\n","\n","def _fit_embeddings(A, dim, seed):\n","    \"\"\"SVD on adjacency → embedding matrix.\"\"\"\n","    set_all_seeds(seed)\n","    svd = TruncatedSVD(n_components=dim, random_state=seed)\n","    emb = svd.fit_transform(A.toarray().astype(np.float64))\n","    # L2-normalise rows\n","    norms = np.linalg.norm(emb, axis=1, keepdims=True)\n","    norms[norms == 0] = 1.0\n","    emb = emb / norms\n","    return emb\n","\n","def _top_k_neighbors(emb, anchor_idx, k):\n","    \"\"\"Return indices of k nearest neighbours (cosine sim) excluding self.\"\"\"\n","    sims = emb @ emb[anchor_idx]\n","    sims[anchor_idx] = -2.0   # exclude self\n","    return set(np.argsort(sims)[-k:].tolist())\n","\n","# ---------- CAPSULE A MAIN ----------\n","set_all_seeds(GLOBAL_CONFIG[\"seed\"])\n","cfg = GLOBAL_CONFIG\n","all_node_ids_ordered = list(G.nodes())   # stable order\n","\n","# (1) Build adjacency with TRAIN edges only\n","A_train = _build_adjacency(G, all_node_ids_ordered,\n","                           edge_indices=SPLIT_MANIFEST[\"embeddings\"][\"train_edge_indices\"],\n","                           edge_df_ref=edge_df)\n","emb_base = _fit_embeddings(A_train, cfg[\"svd_n_components\"], cfg[\"seed\"])\n","print(f\"   Embeddings shape: {emb_base.shape}\")\n","\n","# (2) Link-prediction proxy on holdout edges (AUC — secondary metric)\n","holdout_edges = edge_df.iloc[SPLIT_MANIFEST[\"embeddings\"][\"holdout_edge_indices\"]]\n","idx_map = {nid: i for i, nid in enumerate(all_node_ids_ordered)}\n","\n","pos_scores, neg_scores = [], []\n","for _, row in holdout_edges.iterrows():\n","    si, di = idx_map.get(row[\"src\"]), idx_map.get(row[\"dst\"])\n","    if si is not None and di is not None:\n","        pos_scores.append(float(emb_base[si] @ emb_base[di]))\n","# negative samples: random non-edges\n","set_all_seeds(cfg[\"seed\"])\n","for _ in range(len(pos_scores)):\n","    ri, rj = np.random.randint(0, len(all_node_ids_ordered), size=2)\n","    neg_scores.append(float(emb_base[ri] @ emb_base[rj]))\n","\n","y_true  = [1]*len(pos_scores) + [0]*len(neg_scores)\n","y_score = pos_scores + neg_scores\n","try:\n","    link_pred_auc = round(float(roc_auc_score(y_true, y_score)), 4)\n","except Exception:\n","    link_pred_auc = None\n","\n","# (3) Nearest-neighbour hypotheses for a few anchor nodes\n","set_all_seeds(cfg[\"seed\"])\n","anchor_indices = np.random.choice(len(all_node_ids_ordered),\n","                                  size=min(cfg[\"anchor_nodes_k\"], len(all_node_ids_ordered)),\n","                                  replace=False).tolist()\n","base_neighbors = {}   # anchor_idx → set of k-neighbor indices\n","for ai in anchor_indices:\n","    base_neighbors[ai] = _top_k_neighbors(emb_base, ai, cfg[\"neighbor_k\"])\n","\n","# ---------- (4) STABILITY / SENSITIVITY TESTS ----------\n","perturbation_results = []\n","\n","def _perturb_and_eval(label, perturbed_A):\n","    \"\"\"Refit embeddings on perturbed adjacency, compute overlap & drift.\"\"\"\n","    emb_p = _fit_embeddings(perturbed_A, cfg[\"svd_n_components\"], cfg[\"seed\"])\n","    overlaps, drifts = [], []\n","    for ai in anchor_indices:\n","        nbrs_p = _top_k_neighbors(emb_p, ai, cfg[\"neighbor_k\"])\n","        overlap = len(base_neighbors[ai] & nbrs_p) / cfg[\"neighbor_k\"]\n","        drift   = float(1.0 - emb_base[ai] @ emb_p[ai])   # cosine distance\n","        overlaps.append(overlap)\n","        drifts.append(drift)\n","    mean_overlap = float(np.mean(overlaps))\n","    mean_drift   = float(np.mean(drifts))\n","    perturbation_results.append({\n","        \"perturbation\": label,\n","        \"mean_neighbor_overlap_at_k\": round(mean_overlap, 4),\n","        \"mean_cosine_drift\": round(mean_drift, 4),\n","        \"per_anchor_overlaps\": [round(x, 4) for x in overlaps],\n","        \"per_anchor_drifts\":   [round(x, 4) for x in drifts]\n","    })\n","    return mean_overlap, mean_drift\n","\n","# (a) Edge dropout\n","set_all_seeds(cfg[\"seed\"])\n","train_idx = SPLIT_MANIFEST[\"embeddings\"][\"train_edge_indices\"]\n","keep_mask = np.random.rand(len(train_idx)) > cfg[\"stability_edge_dropout_p\"]\n","dropped_idx = [idx for idx, keep in zip(train_idx, keep_mask) if keep]\n","A_dropout = _build_adjacency(G, all_node_ids_ordered,\n","                             edge_indices=dropped_idx, edge_df_ref=edge_df)\n","_perturb_and_eval(\"edge_dropout\", A_dropout)\n","\n","# (b) Edge noise: add random spurious edges\n","set_all_seeds(cfg[\"seed\"])\n","noise_rows, noise_cols, noise_vals = [], [], []\n","n_nodes = len(all_node_ids_ordered)\n","for _ in range(cfg[\"stability_noise_edges\"]):\n","    ri, rj = np.random.randint(0, n_nodes, size=2)\n","    noise_rows.extend([ri, rj]); noise_cols.extend([rj, ri]); noise_vals.extend([1.0, 1.0])\n","A_noise = A_train + sp.csr_matrix((noise_vals, (noise_rows, noise_cols)), shape=(n_nodes, n_nodes))\n","_perturb_and_eval(\"edge_noise\", A_noise)\n","\n","# (c) Time-window shift: exclude edges older than cutoff\n","cutoff_date = now - timedelta(days=cfg[\"stability_time_cutoff_days\"])\n","time_filtered_idx = []\n","for ei in train_idx:\n","    row = edge_df.iloc[ei]\n","    try:\n","        ts = datetime.fromisoformat(row[\"timestamp\"].replace(\"Z\", \"+00:00\"))\n","        if ts >= cutoff_date:\n","            time_filtered_idx.append(ei)\n","    except Exception:\n","        time_filtered_idx.append(ei)   # keep if unparseable\n","A_time = _build_adjacency(G, all_node_ids_ordered,\n","                          edge_indices=time_filtered_idx, edge_df_ref=edge_df)\n","_perturb_and_eval(\"time_window_shift\", A_time)\n","\n","# ---------- (5) GUARDRAILS ----------\n","emb_interpretation_allowed = True\n","emb_guardrail_events = []\n","\n","for pr in perturbation_results:\n","    if pr[\"mean_neighbor_overlap_at_k\"] < cfg[\"neighbor_overlap_min\"]:\n","        emb_interpretation_allowed = False\n","        emb_guardrail_events.append({\n","            \"trigger\": \"neighbor_overlap_below_threshold\",\n","            \"perturbation\": pr[\"perturbation\"],\n","            \"value\": pr[\"mean_neighbor_overlap_at_k\"],\n","            \"threshold\": cfg[\"neighbor_overlap_min\"],\n","            \"action\": \"interpretation_blocked_for_embeddings\"\n","        })\n","    if pr[\"mean_cosine_drift\"] > cfg[\"score_drift_max\"]:\n","        emb_interpretation_allowed = False\n","        emb_guardrail_events.append({\n","            \"trigger\": \"cosine_drift_above_threshold\",\n","            \"perturbation\": pr[\"perturbation\"],\n","            \"value\": pr[\"mean_cosine_drift\"],\n","            \"threshold\": cfg[\"score_drift_max\"],\n","            \"action\": \"interpretation_blocked_for_embeddings\"\n","        })\n","\n","GUARDRAILS_EVENTS.extend(emb_guardrail_events)\n","SENSITIVITY_DATA[\"embeddings\"] = perturbation_results\n","\n","# ---------- (6) VISUALIZATIONS ----------\n","# --- 6a. 2D embedding projection (PCA) coloured by node_type ---\n","from sklearn.decomposition import PCA\n","pca = PCA(n_components=2, random_state=cfg[\"seed\"])\n","emb_2d = pca.fit_transform(emb_base)\n","\n","type_colors = {\"account\": \"#4A90D9\", \"vendor\": \"#E67E22\", \"device\": \"#2ECC71\"}\n","fig, ax = plt.subplots(figsize=(10, 7))\n","for ntype, color in type_colors.items():\n","    mask = np.array([G.nodes[nid].get(\"node_type\") == ntype for nid in all_node_ids_ordered])\n","    ax.scatter(emb_2d[mask, 0], emb_2d[mask, 1],\n","               c=color, label=ntype, s=18, alpha=0.65, edgecolors=\"none\")\n","# mark hub and connector\n","hub_i  = all_node_ids_ordered.index(f\"acct_{cfg['edge_params']['hub_account_id']:04d}\")\n","conn_i = all_node_ids_ordered.index(f\"acct_{cfg['edge_params']['ambiguous_connector_id']:04d}\")\n","ax.scatter(*emb_2d[hub_i],  marker=\"*\", s=250, c=\"red\",    zorder=5, edgecolors=\"k\", label=\"Hub (spurious centrality)\")\n","ax.scatter(*emb_2d[conn_i], marker=\"D\", s=120, c=\"purple\", zorder=5, edgecolors=\"k\", label=\"Connector (proxy risk)\")\n","ax.legend(fontsize=9, framealpha=0.9)\n","ax.set_title(\"Capsule A — Node Embeddings (PCA 2D)\\n(Synthetic · Not verified)\", fontsize=13, fontweight=\"bold\")\n","ax.set_xlabel(\"PC 1\"); ax.set_ylabel(\"PC 2\")\n","plt.tight_layout()\n","plt.savefig(os.path.join(ARTIFACT_ROOT, \"capsuleA_embedding_pca.png\"), dpi=130, bbox_inches=\"tight\")\n","plt.show()\n","\n","# --- 6b. Stability plot: neighbor overlap@k across perturbations ---\n","perturb_labels = [pr[\"perturbation\"] for pr in perturbation_results]\n","overlap_vals   = [pr[\"mean_neighbor_overlap_at_k\"] for pr in perturbation_results]\n","drift_vals     = [pr[\"mean_cosine_drift\"] for pr in perturbation_results]\n","\n","fig, axes = plt.subplots(1, 2, figsize=(13, 4.5))\n","# overlap\n","axes[0].bar(perturb_labels, overlap_vals, color=[\"#5B9BD5\",\"#ED7D31\",\"#A5A5A5\"], edgecolor=\"k\", linewidth=0.8)\n","axes[0].axhline(cfg[\"neighbor_overlap_min\"], color=\"red\", linestyle=\"--\", linewidth=1.5, label=f\"Threshold ({cfg['neighbor_overlap_min']})\")\n","axes[0].set_ylabel(\"Mean Neighbor Overlap @ k\")\n","axes[0].set_title(\"Stability: Neighbor Overlap\\n(Not verified)\", fontsize=11, fontweight=\"bold\")\n","axes[0].legend(fontsize=8)\n","axes[0].set_ylim(0, 1.05)\n","# drift\n","axes[1].bar(perturb_labels, drift_vals, color=[\"#5B9BD5\",\"#ED7D31\",\"#A5A5A5\"], edgecolor=\"k\", linewidth=0.8)\n","axes[1].axhline(cfg[\"score_drift_max\"], color=\"red\", linestyle=\"--\", linewidth=1.5, label=f\"Threshold ({cfg['score_drift_max']})\")\n","axes[1].set_ylabel(\"Mean Cosine Drift\")\n","axes[1].set_title(\"Stability: Cosine Drift\\n(Not verified)\", fontsize=11, fontweight=\"bold\")\n","axes[1].legend(fontsize=8)\n","axes[1].set_ylim(0, max(drift_vals + [cfg[\"score_drift_max\"]]) * 1.3)\n","plt.tight_layout()\n","plt.savefig(os.path.join(ARTIFACT_ROOT, \"capsuleA_stability.png\"), dpi=130, bbox_inches=\"tight\")\n","plt.show()\n","\n","# ---------- (7) UPDATE RISK LOG ----------\n","RISK_LOG_EVENTS.append({\n","    \"capsule\": \"A_embeddings\",\n","    \"risk_category\": \"proxy_dominance\",\n","    \"detail\": f\"Node {cfg['edge_params']['ambiguous_connector_id']:04d} has high shared_device degree. \"\n","              \"shared_device edges are ambiguous proxies — interpretation must not treat connectivity as intent.\",\n","    \"severity\": \"warning\"\n","})\n","RISK_LOG_EVENTS.append({\n","    \"capsule\": \"A_embeddings\",\n","    \"risk_category\": \"spurious_centrality\",\n","    \"detail\": f\"Hub node acct_{cfg['edge_params']['hub_account_id']:04d} has artificially high degree. \"\n","              \"Embedding proximity to hub does NOT indicate shared risk.\",\n","    \"severity\": \"warning\"\n","})\n","RISK_LOG_EVENTS.append({\n","    \"capsule\": \"A_embeddings\",\n","    \"risk_category\": \"boundary_sensitivity\",\n","    \"detail\": \"Time-window shift perturbation was applied. If overlap/drift thresholds are breached, \"\n","              \"interpretation is blocked to prevent boundary-dependent conclusions.\",\n","    \"severity\": \"info\"\n","})\n","\n","# ---------- Save train metrics for embeddings ----------\n","TRAIN_METRICS[\"embeddings\"] = {\n","    \"method\": \"adjacency_svd\",\n","    \"n_components\": cfg[\"svd_n_components\"],\n","    \"train_edges_used\": len(SPLIT_MANIFEST[\"embeddings\"][\"train_edge_indices\"]),\n","    \"link_pred_auc_proxy\": link_pred_auc,\n","    \"interpretation_allowed\": emb_interpretation_allowed,\n","    \"verification_status\": \"Not verified\"\n","}\n","\n","print(f\"✅ Cell 6 complete — Capsule A (Embeddings)\")\n","print(f\"   Link-pred AUC (proxy, secondary): {link_pred_auc}\")\n","print(f\"   Interpretation allowed: {emb_interpretation_allowed}\")\n","print(f\"   Guardrail events: {len(emb_guardrail_events)}\")\n"],"metadata":{"id":"HR3pyRxjjV3p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769956065231,"user_tz":360,"elapsed":4426,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"9578abd5-7eea-41c8-bb58-fe5cc5aa660c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   Embeddings shape: (1250, 32)\n","✅ Cell 6 complete — Capsule A (Embeddings)\n","   Link-pred AUC (proxy, secondary): 0.4514\n","   Interpretation allowed: False\n","   Guardrail events: 4\n"]}]},{"cell_type":"markdown","source":["##7.GOVERNED GRAPH NEURAL NETWORK"],"metadata":{"id":"hnqxNRdOigzp"}},{"cell_type":"markdown","source":["###7.1.OVERVIEW"],"metadata":{"id":"9N3hP5ksikQZ"}},{"cell_type":"markdown","source":["**Cell 7 — Governed Capsule B: Graph Neural Network**\n","\n","This cell builds and trains a two-layer graph convolutional network from scratch in PyTorch, with no graph neural network library. Every operation — adjacency construction, normalization, message passing, gradient updates — is written explicitly. This is not an accident of tooling. It is a governance decision. When every step is visible in the code, every step can be audited. A model imported as a black-box function cannot be governed in the same way.\n","\n","The adjacency matrix is filtered before it reaches the model. Only transaction and co\\_supply edges are permitted. shared\\_device edges are excluded by governance rule, and the exclusion is documented in a propagation constraints artifact with an explicit justification: shared\\_device is an ambiguous proxy channel, and including it in message passing would propagate proxy signals without semantic grounding. This exclusion is the first place in the notebook where a governance rule directly shapes what the model can see. The influence of any single neighbor is also clipped to prevent one node from dominating the aggregation.\n","\n","The node feature matrix includes three columns: two synthetic continuous features and the intentionally proxy-like synth_proxy_device_count. Its inclusion is not an oversight. It is a demonstration. The proxy feature is flagged in the guardrails as requiring human review. A student reading this cell learns that putting a proxy feature into a GNN is not a modeling error that a linter can catch. It is a governance question that only a domain expert can answer.\n","\n","The GNN trains for sixty epochs on a synthetic binary classification task. The training loss decreases. The validation accuracy fluctuates. The test confusion matrix likely shows heavy prediction of the majority class. None of these results are surprising, because the label was assigned at random. The model is fitting noise in the neighborhood structure. The metrics are not evidence of capability. They are evidence that the pipeline executed correctly and that the governance workflow measured what it was designed to measure.\n","\n","Sensitivity tests re-evaluate the trained model on perturbed adjacencies: one with edges dropped, one with the time window shifted. Score drift and prediction flip rate are measured. If either breaches its threshold, interpretation is blocked. The proxy feature warning is recorded as a guardrail event regardless of whether the sensitivity tests pass or fail. This ensures that the proxy risk is visible in the audit trail even when the model otherwise performs stably. A student reading this cell learns that stability and proxy risk are independent governance checks, and that passing one does not neutralize the other.\n"],"metadata":{"id":"MzaMhrkuimFO"}},{"cell_type":"markdown","source":["###7.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"epotbIpqimYU"}},{"cell_type":"code","source":["\n","# =============================================================================\n","# CELL 7 — Governed Capsule B: Graph Neural Network (End-to-End + Visuals)\n","# =============================================================================\n","\n","# ---------- 1. Build FILTERED adjacency (only allowed edge types) ----------\n","allowed_etypes = set(cfg[\"gnn_allowed_edge_types\"])   # excludes shared_device\n","\n","def _build_gnn_adjacency(graph, node_list, allowed_types, edge_df_src=None, time_filter=None):\n","    \"\"\"\n","    Build normalised adjacency for GCN.\n","    Only edges whose edge_type is in allowed_types are included.\n","    If time_filter (datetime) is given, only edges with timestamp >= time_filter.\n","    Clips influence to gnn_influence_clip after D^{-1/2} A D^{-1/2}.\n","    \"\"\"\n","    n = len(node_list)\n","    idx_map = {nid: i for i, nid in enumerate(node_list)}\n","    rows, cols = [], []\n","\n","    for u, v, d in graph.edges(data=True):\n","        if d.get(\"edge_type\") not in allowed_types:\n","            continue\n","        if time_filter is not None:\n","            try:\n","                ts = datetime.fromisoformat(d.get(\"timestamp\", \"\").replace(\"Z\", \"+00:00\"))\n","                if ts < time_filter:\n","                    continue\n","            except Exception:\n","                pass\n","        if u in idx_map and v in idx_map:\n","            si, di = idx_map[u], idx_map[v]\n","            rows.extend([si, di]); cols.extend([di, si])   # symmetrise\n","\n","    # add self-loops\n","    rows.extend(range(n)); cols.extend(range(n))\n","    vals = np.ones(len(rows))\n","    A = sp.csr_matrix((vals, (rows, cols)), shape=(n, n))\n","\n","    # D^{-1/2} A D^{-1/2} normalisation\n","    deg = np.array(A.sum(axis=1)).flatten()\n","    deg_inv_sqrt = np.zeros_like(deg)\n","    nonzero = deg > 0\n","    deg_inv_sqrt[nonzero] = 1.0 / np.sqrt(deg[nonzero])\n","    D_inv_sqrt = sp.diags(deg_inv_sqrt)\n","    A_norm = D_inv_sqrt @ A @ D_inv_sqrt\n","\n","    # influence clipping\n","    clip = cfg[\"gnn_influence_clip\"]\n","    A_dense = A_norm.toarray()\n","    A_dense = np.clip(A_dense, 0, clip)\n","    return torch.tensor(A_dense, dtype=torch.float32)\n","\n","set_all_seeds(cfg[\"seed\"])\n","A_gnn_base = _build_gnn_adjacency(G, all_node_ids_ordered, allowed_etypes)\n","\n","# ---------- 2. Node features ----------\n","# Use synth_feature_1, synth_feature_2, and intentionally the proxy feature\n","feature_cols = [\"synth_feature_1\", \"synth_feature_2\", \"synth_proxy_device_count\"]\n","# Build feature matrix aligned with all_node_ids_ordered\n","node_id_to_row = dict(zip(node_df[\"node_id\"], node_df.index))\n","X_np = np.zeros((len(all_node_ids_ordered), len(feature_cols)), dtype=np.float32)\n","for i, nid in enumerate(all_node_ids_ordered):\n","    if nid in node_id_to_row:\n","        row_idx = node_id_to_row[nid]\n","        X_np[i] = node_df.loc[row_idx, feature_cols].values.astype(np.float32)\n","# z-score normalise\n","X_mean = X_np.mean(axis=0)\n","X_std  = X_np.std(axis=0)\n","X_std[X_std == 0] = 1.0\n","X_np = (X_np - X_mean) / X_std\n","X_tensor = torch.tensor(X_np, dtype=torch.float32)\n","\n","# ---------- 3. Labels (accounts only) ----------\n","# Map node positions to labels; non-account nodes get label -1 (masked)\n","y_np = np.full(len(all_node_ids_ordered), -1, dtype=np.int64)\n","for i, nid in enumerate(all_node_ids_ordered):\n","    if nid in node_id_to_row:\n","        row_idx = node_id_to_row[nid]\n","        if node_df.loc[row_idx, \"node_type\"] == \"account\":\n","            y_np[i] = int(node_df.loc[row_idx, \"review_flag\"])\n","y_tensor = torch.tensor(y_np, dtype=torch.long)\n","\n","# Train/val/test masks (node-level, based on all_node_ids_ordered positions)\n","nid_to_pos = {nid: i for i, nid in enumerate(all_node_ids_ordered)}\n","train_mask = torch.zeros(len(all_node_ids_ordered), dtype=torch.bool)\n","val_mask   = torch.zeros(len(all_node_ids_ordered), dtype=torch.bool)\n","test_mask  = torch.zeros(len(all_node_ids_ordered), dtype=torch.bool)\n","\n","# gnn_train_idx etc. are DataFrame indices; map to graph positions\n","for df_idx in SPLIT_MANIFEST[\"gnn\"][\"train_indices\"]:\n","    nid = node_df.loc[df_idx, \"node_id\"]\n","    if nid in nid_to_pos:\n","        train_mask[nid_to_pos[nid]] = True\n","for df_idx in SPLIT_MANIFEST[\"gnn\"][\"val_indices\"]:\n","    nid = node_df.loc[df_idx, \"node_id\"]\n","    if nid in nid_to_pos:\n","        val_mask[nid_to_pos[nid]] = True\n","for df_idx in SPLIT_MANIFEST[\"gnn\"][\"test_indices\"]:\n","    nid = node_df.loc[df_idx, \"node_id\"]\n","    if nid in nid_to_pos:\n","        test_mask[nid_to_pos[nid]] = True\n","\n","# ---------- 4. Minimal GCN model ----------\n","class MinimalGCN(nn.Module):\n","    \"\"\"2-layer GCN: input → hidden (ReLU) → output (2 classes).\"\"\"\n","    def __init__(self, in_dim, hidden_dim, out_dim):\n","        super().__init__()\n","        self.W1 = nn.Parameter(torch.randn(in_dim, hidden_dim) * 0.1)\n","        self.W2 = nn.Parameter(torch.randn(hidden_dim, out_dim) * 0.1)\n","        self.bias1 = nn.Parameter(torch.zeros(hidden_dim))\n","        self.bias2 = nn.Parameter(torch.zeros(out_dim))\n","\n","    def forward(self, X, A):\n","        # Layer 1: A·X·W1 + b1, ReLU\n","        H = torch.relu(A @ X @ self.W1 + self.bias1)\n","        # Layer 2: A·H·W2 + b2  (no activation — logits)\n","        out = A @ H @ self.W2 + self.bias2\n","        return out\n","\n","set_all_seeds(cfg[\"seed\"])\n","model = MinimalGCN(in_dim=len(feature_cols),\n","                   hidden_dim=cfg[\"gnn_hidden_dim\"],\n","                   out_dim=2)\n","optimizer = optim.Adam(model.parameters(), lr=cfg[\"gnn_lr\"],\n","                       weight_decay=cfg[\"gnn_weight_decay\"])\n","criterion = nn.CrossEntropyLoss()\n","\n","# ---------- 5. Training loop ----------\n","train_losses, val_accs = [], []\n","\n","for epoch in range(cfg[\"gnn_epochs\"]):\n","    model.train()\n","    optimizer.zero_grad()\n","    logits = model(X_tensor, A_gnn_base)\n","    loss = criterion(logits[train_mask], y_tensor[train_mask])\n","    loss.backward()\n","    optimizer.step()\n","    train_losses.append(round(float(loss.item()), 6))\n","\n","    # validation accuracy\n","    model.eval()\n","    with torch.no_grad():\n","        val_logits = model(X_tensor, A_gnn_base)\n","        val_preds  = val_logits[val_mask].argmax(dim=1)\n","        val_acc    = float((val_preds == y_tensor[val_mask]).float().mean().item())\n","    val_accs.append(round(val_acc, 4))\n","\n","# ---------- 6. Evaluation on test set ----------\n","model.eval()\n","with torch.no_grad():\n","    test_logits = model(X_tensor, A_gnn_base)\n","    test_preds  = test_logits[test_mask].argmax(dim=1).numpy()\n","    test_true   = y_tensor[test_mask].numpy()\n","\n","test_acc = round(float(accuracy_score(test_true, test_preds)), 4)\n","test_f1  = round(float(f1_score(test_true, test_preds, zero_division=0)), 4)\n","cm = confusion_matrix(test_true, test_preds)\n","\n","# ---------- 7. Write propagation_constraints.json ----------\n","PROPAGATION_CONSTRAINTS = {\n","    \"run_id\": RUN_ID,\n","    \"max_hops\": cfg[\"gnn_max_hops\"],\n","    \"gcn_layers\": cfg[\"gnn_max_hops\"],\n","    \"allowed_edge_types\": list(allowed_etypes),\n","    \"excluded_edge_types\": [\"shared_device\"],\n","    \"exclusion_justification\": \"shared_device is an ambiguous proxy channel. Including it in message-passing \"\n","                                \"would propagate proxy signals without semantic grounding.\",\n","    \"influence_clip\": cfg[\"gnn_influence_clip\"],\n","    \"normalisation\": \"symmetric D^{-1/2} A D^{-1/2}\",\n","    \"self_loops\": \"included (standard GCN practice)\",\n","    \"verification_status\": \"Not verified\"\n","}\n","stable_json_dump(PATHS[\"propagation_constraints\"], PROPAGATION_CONSTRAINTS)\n","\n","# ---------- 8. SENSITIVITY TESTS ----------\n","gnn_perturbation_results = []\n","\n","def _eval_gnn_on_adj(A_perturbed, label):\n","    \"\"\"Evaluate trained model on a perturbed adjacency. Measure score drift + flip rate.\"\"\"\n","    model.eval()\n","    with torch.no_grad():\n","        base_logits = model(X_tensor, A_gnn_base)\n","        pert_logits = model(X_tensor, A_perturbed)\n","        base_preds  = base_logits[test_mask].argmax(dim=1).numpy()\n","        pert_preds  = pert_logits[test_mask].argmax(dim=1).numpy()\n","        # score drift: mean absolute difference of softmax probs\n","        base_probs = torch.softmax(base_logits[test_mask], dim=1).numpy()\n","        pert_probs = torch.softmax(pert_logits[test_mask], dim=1).numpy()\n","        score_drift = float(np.abs(base_probs - pert_probs).mean())\n","        flip_rate   = float((base_preds != pert_preds).mean())\n","    gnn_perturbation_results.append({\n","        \"perturbation\": label,\n","        \"mean_score_drift\": round(score_drift, 4),\n","        \"prediction_flip_rate\": round(flip_rate, 4)\n","    })\n","    return score_drift, flip_rate\n","\n","# (a) Edge dropout — rebuild adjacency with dropout\n","set_all_seeds(cfg[\"seed\"])\n","# We rebuild from graph but randomly drop edges\n","G_dropped = G.copy()\n","edges_to_drop = []\n","for u, v, k, d in list(G_dropped.edges(keys=True, data=True)):\n","    if d.get(\"edge_type\") in allowed_etypes and np.random.rand() < cfg[\"stability_edge_dropout_p\"]:\n","        edges_to_drop.append((u, v, k))\n","for u, v, k in edges_to_drop:\n","    G_dropped.remove_edge(u, v, k)\n","A_gnn_dropout = _build_gnn_adjacency(G_dropped, all_node_ids_ordered, allowed_etypes)\n","_eval_gnn_on_adj(A_gnn_dropout, \"edge_dropout\")\n","\n","# (b) Time-window shift\n","time_cutoff = now - timedelta(days=cfg[\"stability_time_cutoff_days\"])\n","A_gnn_time = _build_gnn_adjacency(G, all_node_ids_ordered, allowed_etypes, time_filter=time_cutoff)\n","_eval_gnn_on_adj(A_gnn_time, \"time_window_shift\")\n","\n","SENSITIVITY_DATA[\"gnn\"] = gnn_perturbation_results\n","\n","# ---------- 9. GUARDRAILS ----------\n","gnn_interpretation_allowed = True\n","gnn_guardrail_events = []\n","\n","for pr in gnn_perturbation_results:\n","    if pr[\"mean_score_drift\"] > cfg[\"score_drift_max\"]:\n","        gnn_interpretation_allowed = False\n","        gnn_guardrail_events.append({\n","            \"trigger\": \"score_drift_above_threshold\",\n","            \"perturbation\": pr[\"perturbation\"],\n","            \"value\": pr[\"mean_score_drift\"],\n","            \"threshold\": cfg[\"score_drift_max\"],\n","            \"action\": \"interpretation_blocked_for_gnn\"\n","        })\n","    if pr[\"prediction_flip_rate\"] > 0.20:   # hard-coded flip-rate guardrail\n","        gnn_interpretation_allowed = False\n","        gnn_guardrail_events.append({\n","            \"trigger\": \"prediction_flip_rate_high\",\n","            \"perturbation\": pr[\"perturbation\"],\n","            \"value\": pr[\"prediction_flip_rate\"],\n","            \"threshold\": 0.20,\n","            \"action\": \"interpretation_blocked_for_gnn\"\n","        })\n","\n","# Proxy feature warning: synth_proxy_device_count is intentionally proxy-like\n","gnn_guardrail_events.append({\n","    \"trigger\": \"proxy_feature_warning\",\n","    \"feature\": \"synth_proxy_device_count\",\n","    \"detail\": \"This feature is a proxy for device connectivity. Using it in GNN without \"\n","              \"domain validation risks proxy dominance. Flagged for human review.\",\n","    \"severity\": \"warning\",\n","    \"action\": \"flagged_for_review\"\n","})\n","\n","GUARDRAILS_EVENTS.extend(gnn_guardrail_events)\n","\n","# ---------- 10. VISUALIZATIONS ----------\n","fig, axes = plt.subplots(1, 3, figsize=(17, 4.5))\n","\n","# --- 10a. Training loss + validation accuracy ---\n","ax = axes[0]\n","ax.plot(train_losses, color=\"#4A90D9\", linewidth=1.8, label=\"Train Loss\")\n","ax.set_ylabel(\"Loss\", color=\"#4A90D9\"); ax.set_xlabel(\"Epoch\")\n","ax.set_title(\"GNN Training Loss\\n(Not verified)\", fontsize=11, fontweight=\"bold\")\n","ax2 = ax.twinx()\n","ax2.plot(val_accs, color=\"#E67E22\", linewidth=1.5, linestyle=\"--\", label=\"Val Accuracy\")\n","ax2.set_ylabel(\"Val Accuracy\", color=\"#E67E22\")\n","ax.legend(loc=\"upper left\", fontsize=8)\n","ax2.legend(loc=\"lower left\", fontsize=8)\n","\n","# --- 10b. Confusion matrix ---\n","ax = axes[1]\n","im = ax.imshow(cm, cmap=\"Blues\", aspect=\"auto\")\n","ax.set_xticks([0, 1]); ax.set_xticklabels([\"Pred 0\", \"Pred 1\"])\n","ax.set_yticks([0, 1]); ax.set_yticklabels([\"True 0\", \"True 1\"])\n","for i in range(2):\n","    for j in range(2):\n","        ax.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\",\n","                color=\"white\" if cm[i, j] > cm.max()/2 else \"black\", fontsize=14)\n","ax.set_title(\"Confusion Matrix (Test)\\n(Synthetic · Not verified)\", fontsize=11, fontweight=\"bold\")\n","fig.colorbar(im, ax=ax, shrink=0.8)\n","\n","# --- 10c. Sensitivity: prediction flip rate ---\n","ax = axes[2]\n","perturb_names = [pr[\"perturbation\"] for pr in gnn_perturbation_results]\n","flip_rates    = [pr[\"prediction_flip_rate\"] for pr in gnn_perturbation_results]\n","drift_vals_g  = [pr[\"mean_score_drift\"] for pr in gnn_perturbation_results]\n","x_pos = np.arange(len(perturb_names))\n","width = 0.35\n","bars1 = ax.bar(x_pos - width/2, flip_rates, width, color=\"#E74C3C\", edgecolor=\"k\", linewidth=0.8, label=\"Flip Rate\")\n","bars2 = ax.bar(x_pos + width/2, drift_vals_g, width, color=\"#9B59B6\", edgecolor=\"k\", linewidth=0.8, label=\"Score Drift\")\n","ax.axhline(0.20, color=\"red\", linestyle=\"--\", linewidth=1, label=\"Flip threshold (0.20)\")\n","ax.axhline(cfg[\"score_drift_max\"], color=\"purple\", linestyle=\"--\", linewidth=1, label=f\"Drift threshold ({cfg['score_drift_max']})\")\n","ax.set_xticks(x_pos); ax.set_xticklabels(perturb_names, fontsize=9)\n","ax.set_ylabel(\"Rate\")\n","ax.set_title(\"GNN Sensitivity\\n(Not verified)\", fontsize=11, fontweight=\"bold\")\n","ax.legend(fontsize=7, loc=\"upper right\")\n","ax.set_ylim(0, max(flip_rates + drift_vals_g + [0.25, cfg[\"score_drift_max\"]]) * 1.35)\n","\n","plt.tight_layout()\n","plt.savefig(os.path.join(ARTIFACT_ROOT, \"capsuleB_gnn_visuals.png\"), dpi=130, bbox_inches=\"tight\")\n","plt.show()\n","\n","# ---------- 11. UPDATE RISK LOG ----------\n","RISK_LOG_EVENTS.append({\n","    \"capsule\": \"B_gnn\",\n","    \"risk_category\": \"contagion_propagation\",\n","    \"detail\": f\"GCN uses {cfg['gnn_max_hops']}-hop message passing. Information (and bias) propagates \"\n","              \"through allowed edge types. Structural amplification is possible.\",\n","    \"severity\": \"warning\"\n","})\n","RISK_LOG_EVENTS.append({\n","    \"capsule\": \"B_gnn\",\n","    \"risk_category\": \"boundary_errors\",\n","    \"detail\": \"Time-window shift test changes which edges are in-scope. If predictions shift significantly, \"\n","              \"the model's output is boundary-dependent and unreliable for fixed decisions.\",\n","    \"severity\": \"warning\"\n","})\n","RISK_LOG_EVENTS.append({\n","    \"capsule\": \"B_gnn\",\n","    \"risk_category\": \"accountability_diffusion\",\n","    \"detail\": \"GNN scores are functions of neighbourhood structure. Attributing a score to a single node \"\n","              \"obscures the collective origin of the signal. Human review must trace provenance.\",\n","    \"severity\": \"warning\"\n","})\n","RISK_LOG_EVENTS.append({\n","    \"capsule\": \"B_gnn\",\n","    \"risk_category\": \"proxy_dominance\",\n","    \"detail\": \"synth_proxy_device_count is included as a node feature. This is a proxy for device connectivity \"\n","              \"and may dominate gradient flow. Requires domain sign-off before any real use.\",\n","    \"severity\": \"warning\"\n","})\n","\n","# ---------- Save metrics ----------\n","TRAIN_METRICS[\"gnn\"] = {\n","    \"method\": \"manual_gcn_pytorch\",\n","    \"layers\": cfg[\"gnn_max_hops\"],\n","    \"hidden_dim\": cfg[\"gnn_hidden_dim\"],\n","    \"epochs\": cfg[\"gnn_epochs\"],\n","    \"final_train_loss\": train_losses[-1],\n","    \"train_losses_per_epoch\": train_losses,\n","    \"val_accs_per_epoch\": val_accs,\n","    \"verification_status\": \"Not verified\"\n","}\n","EVAL_METRICS[\"gnn\"] = {\n","    \"test_accuracy\": test_acc,\n","    \"test_f1\": test_f1,\n","    \"confusion_matrix\": cm.tolist(),\n","    \"interpretation_allowed\": gnn_interpretation_allowed,\n","    \"verification_status\": \"Not verified\"\n","}\n","\n","print(f\"✅ Cell 7 complete — Capsule B (GNN)\")\n","print(f\"   Test accuracy: {test_acc} | Test F1: {test_f1}\")\n","print(f\"   Interpretation allowed: {gnn_interpretation_allowed}\")\n","print(f\"   Guardrail events: {len(gnn_guardrail_events)}\")\n","\n"],"metadata":{"id":"BTVBYbY4jXuo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769956177435,"user_tz":360,"elapsed":6626,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"8f329fc0-7ffd-4ef6-e67b-f67aebd5412d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Cell 7 complete — Capsule B (GNN)\n","   Test accuracy: 0.9375 | Test F1: 0.0\n","   Interpretation allowed: True\n","   Guardrail events: 1\n"]}]},{"cell_type":"markdown","source":["##8.GUARDRAILS REPORT AND DETERMINISTIC DECISION LOGIC"],"metadata":{"id":"BLus531lirIL"}},{"cell_type":"markdown","source":["###8.1.OVERVIEW"],"metadata":{"id":"x0j2JYTnixuf"}},{"cell_type":"markdown","source":["**Cell 8 — Guardrails Report + Deterministic Decision Logic**\n","\n","This cell is where the notebook makes its decision. Not a recommendation. Not a suggestion. A deterministic, auditable verdict on whether the outputs of Capsules A and B are permitted to be interpreted. The logic is three rules, evaluated in order. If schema validation failed, the status is \"blocked\" — but if execution reached this cell, validation passed in Cell 5, so this rule is already satisfied. If either capsule failed its stability tests, or if a critical proxy warning was raised, the status is \"abstain.\" Otherwise, the status is \"pass\\_exploratory.\" In every case, required\\_human\\_review is set to True. There is no path through this cell that produces a decision without mandatory human oversight. The decision artifact records not only the final status but the exact rule that produced it, so a reviewer can read the logic without re-running the notebook.\n","\n","The guardrails report consolidates everything that was enforced during the run. It lists allowed uses — exploratory hypothesis generation, human-review prioritization, teaching — and prohibited uses — ranking entities for action, determining eligibility, adverse action, causal claims, normative labeling. It records every enforcement event from both capsules, including stability failures, proxy warnings, and the refusal example that will be added in Cell 10. It states two interpretation rules that govern everything this notebook has produced: structure is not causality, and connectivity is not explanation.\n","\n","The sensitivity report collects the raw perturbation data from both capsules into a single artifact. The thresholds used to evaluate that data are recorded alongside it. A reviewer reading this report can see not only whether each perturbation passed or failed, but the exact values that produced that verdict and the exact thresholds against which they were compared.\n","\n","The train and eval metrics files are written here rather than in the capsule cells because this cell is the consolidation point. It assembles metrics from both capsules into unified files, each stamped \"Not verified.\" The risk log is finalized here as well, collecting all risk events from both capsules under five categories: relational bias and structural amplification, contagion, boundary errors, spurious centrality and proxy dominance, and accountability diffusion. These categories are not generic labels. Each one corresponds to a specific failure mode demonstrated somewhere in the notebook. A human reviewer reading the risk log can trace each event back to the capsule and the code section that produced it, which is the minimum condition for accountability in a governed system.\n"],"metadata":{"id":"CGJ3TUQwi2JO"}},{"cell_type":"markdown","source":["###8.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"SCv1wHOni2cy"}},{"cell_type":"code","source":["\n","\n","# ---------- 1. Guardrails Report ----------\n","GUARDRAILS_REPORT = {\n","    \"run_id\": RUN_ID,\n","    \"timestamp\": now_utc_iso(),\n","    \"allowed_use\": [\n","        \"Exploratory hypothesis generation only.\",\n","        \"Human-review prioritisation (e.g., selecting which hypotheses a domain expert examines first).\",\n","        \"Teaching and governance demonstration.\"\n","    ],\n","    \"prohibited_use\": [\n","        \"Ranking entities for action or investigation.\",\n","        \"Determining eligibility, access, or creditworthiness.\",\n","        \"Adverse action against any individual or entity.\",\n","        \"Automated escalation without human review.\",\n","        \"Causal claims derived from graph structure.\",\n","        \"Normative labelling (e.g., 'suspicious', 'high-risk') without domain validation.\"\n","    ],\n","    \"enforcement_events\": GUARDRAILS_EVENTS,\n","    \"interpretation_rules\": {\n","        \"fact_vs_hypothesis\": \"Any pattern observed in the graph (cluster, centrality, similarity) is a HYPOTHESIS. \"\n","                              \"It is NOT a fact until independently verified by domain experts.\",\n","        \"not_verified_label\": \"All interpretive outputs carry the label 'Not verified'.\",\n","        \"stability_prerequisite\": \"Interpretation is only permitted when stability tests pass for the relevant capsule.\",\n","        \"structure_is_not_causality\": \"Structure is not causality.\",\n","        \"connectivity_is_not_explanation\": \"Connectivity is not explanation.\"\n","    },\n","    \"capsule_status\": {\n","        \"embeddings\": {\n","            \"interpretation_allowed\": emb_interpretation_allowed,\n","            \"guardrail_events_count\": len([e for e in GUARDRAILS_EVENTS if \"embeddings\" in str(e).lower() or e.get(\"trigger\",\"\").startswith(\"neighbor\") or e.get(\"trigger\",\"\").startswith(\"cosine\")])\n","        },\n","        \"gnn\": {\n","            \"interpretation_allowed\": gnn_interpretation_allowed,\n","            \"guardrail_events_count\": len([e for e in GUARDRAILS_EVENTS if \"gnn\" in str(e).lower() or e.get(\"trigger\",\"\").startswith(\"score_drift\") or e.get(\"trigger\",\"\").startswith(\"prediction\") or e.get(\"trigger\",\"\").startswith(\"proxy\")])\n","        }\n","    },\n","    \"verification_status\": \"Not verified\"\n","}\n","stable_json_dump(PATHS[\"guardrails_report\"], GUARDRAILS_REPORT)\n","\n","# ---------- 2. Deterministic Decision Logic ----------\n","# Rules (evaluated in order):\n","#   1. If schema validation failed → \"blocked\"  (handled in Cell 5; if we're here, it passed)\n","#   2. If EITHER capsule fails stability OR proxy_dominance is flagged critical → \"abstain\"\n","#   3. If BOTH capsules pass stability and no critical warnings → \"pass_exploratory\"\n","\n","has_critical_proxy = any(\n","    e.get(\"trigger\") == \"proxy_feature_warning\" and e.get(\"severity\") == \"warning\"\n","    for e in GUARDRAILS_EVENTS\n",")\n","either_capsule_fails = (not emb_interpretation_allowed) or (not gnn_interpretation_allowed)\n","\n","if either_capsule_fails or has_critical_proxy:\n","    overall_status = \"abstain\"\n","else:\n","    overall_status = \"pass_exploratory\"\n","\n","DECISION = {\n","    \"run_id\": RUN_ID,\n","    \"timestamp\": now_utc_iso(),\n","    \"overall_status\": overall_status,\n","    \"interpretation_allowed_embeddings\": emb_interpretation_allowed,\n","    \"interpretation_allowed_gnn\": gnn_interpretation_allowed,\n","    \"required_human_review\": True,\n","    \"verification_status\": \"Not verified\",\n","    \"decision_logic\": {\n","        \"schema_validation_passed\": True,\n","        \"embeddings_stability_passed\": emb_interpretation_allowed,\n","        \"gnn_stability_passed\": gnn_interpretation_allowed,\n","        \"critical_proxy_warning_present\": has_critical_proxy,\n","        \"rule_applied\": (\n","            \"abstain: either capsule failed stability or critical proxy warning present.\"\n","            if (either_capsule_fails or has_critical_proxy)\n","            else \"pass_exploratory: both capsules stable, no critical warnings.\"\n","        )\n","    },\n","    \"notes\": \"This decision gate is DETERMINISTIC. overall_status is derived solely from \"\n","             \"stability thresholds and guardrail triggers — no human input modifies it. \"\n","             \"required_human_review is ALWAYS true regardless of status.\"\n","}\n","stable_json_dump(PATHS[\"decision\"], DECISION)\n","\n","# ---------- 3. Write sensitivity report (both capsules) ----------\n","SENSITIVITY_REPORT = {\n","    \"run_id\": RUN_ID,\n","    \"embeddings\": SENSITIVITY_DATA.get(\"embeddings\", []),\n","    \"gnn\": SENSITIVITY_DATA.get(\"gnn\", []),\n","    \"thresholds\": {\n","        \"neighbor_overlap_min\": cfg[\"neighbor_overlap_min\"],\n","        \"score_drift_max\": cfg[\"score_drift_max\"],\n","        \"prediction_flip_rate_max\": 0.20\n","    },\n","    \"verification_status\": \"Not verified\"\n","}\n","stable_json_dump(PATHS[\"sensitivity_report\"], SENSITIVITY_REPORT)\n","\n","# ---------- 4. Write train_metrics + eval_metrics ----------\n","stable_json_dump(PATHS[\"train_metrics\"], {\n","    \"run_id\": RUN_ID,\n","    \"embeddings\": TRAIN_METRICS.get(\"embeddings\", {}),\n","    \"gnn\": TRAIN_METRICS.get(\"gnn\", {}),\n","    \"verification_status\": \"Not verified\"\n","})\n","stable_json_dump(PATHS[\"eval_metrics\"], {\n","    \"run_id\": RUN_ID,\n","    \"gnn\": EVAL_METRICS.get(\"gnn\", {}),\n","    \"verification_status\": \"Not verified\"\n","})\n","\n","# ---------- 5. Write risk_log ----------\n","RISK_LOG_FINAL = {\n","    \"run_id\": RUN_ID,\n","    \"verification_status\": \"Not verified\",\n","    \"risk_categories_covered\": [\n","        \"relational_bias_structural_amplification\",\n","        \"contagion\",\n","        \"boundary_errors\",\n","        \"spurious_centrality_proxy_dominance\",\n","        \"accountability_diffusion\"\n","    ],\n","    \"events\": RISK_LOG_EVENTS\n","}\n","stable_json_dump(PATHS[\"risk_log\"], RISK_LOG_FINAL)\n","\n","print(\"✅ Cell 8 complete — Guardrails report + Decision gate written.\")\n","print(f\"   Overall status:  {overall_status}\")\n","print(f\"   Embeddings interpretation allowed: {emb_interpretation_allowed}\")\n","print(f\"   GNN interpretation allowed:        {gnn_interpretation_allowed}\")\n","print(f\"   Required human review:             True (always)\")\n"],"metadata":{"id":"s8BmlsxYiw1_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769956330180,"user_tz":360,"elapsed":24,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"a2d2c574-d7f8-47c4-97e1-f470ccffb7c6"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Cell 8 complete — Guardrails report + Decision gate written.\n","   Overall status:  abstain\n","   Embeddings interpretation allowed: False\n","   GNN interpretation allowed:        True\n","   Required human review:             True (always)\n"]}]},{"cell_type":"markdown","source":["##9.MODEL CARDS AND GOVERNANCE MEMOS"],"metadata":{"id":"7JxS9vVQi4yB"}},{"cell_type":"markdown","source":["###9.1.0VERVIEW"],"metadata":{"id":"Wurgn_8fi5-m"}},{"cell_type":"markdown","source":["**Cell 9 — Model Card + Governance Memo**\n","\n","This cell writes the two most important artifacts in the bundle: the model card and the governance memo. These are not technical records of what the code did. They are human-facing documents designed to be read before anything else in the artifact bundle is examined. The conclusion of this notebook says it explicitly: read the governance memo before you read the plots. This cell is where that instruction is earned.\n","\n","The model card covers both capsules under a single document. It states the intended use: teaching, demonstration, hypothesis generation for human review. It states what is out of scope: real-world decisions, production deployment, causal inference, any use without human review. The limitations section is candid. SVD captures only linear spectral structure. The GCN sees only two hops. Class imbalance inflates or deflates metrics. Stability tests cover only three perturbation types. These are not apologies. They are boundaries. A model card that omits its own limitations is not a model card. It is marketing. The edge ambiguity notes describe each edge type and what it does and does not mean. The propagation constraints summary records the hop depth, the permitted edge types, and the influence clip. Two mandatory statements close the document: structure is not causality, and connectivity is not explanation.\n","\n","The governance memo is structured around seven keys that enforce a discipline of separating what is known from what is assumed. Facts\\_provided records objective outcomes: the run identifier, the config hash, graph statistics, validation results, stability outcomes for both capsules, the decision status, and counts of risk and guardrail events. Assumptions lists every interpretive hypothesis the notebook has generated, each prefaced with \"not verified\" and each explicitly stripped of any normative claim. A hypothesis in this notebook is a statement about what the data might indicate. It is not a statement about what is true. Open\\_items is a checklist of things that have not been done: data access approvals, privacy review, bias testing, domain sign-off, proxy feature review, and real-data re-validation. Analysis summarizes what was demonstrated. Draft\\_output states the decision status in plain language. Questions\\_to\\_verify is the reviewer's checklist: eight specific questions, each targeting a governance gap that a human must close before any output moves toward production. This memo is the single artifact a reviewer should read first. Everything else in the bundle supports it.\n"],"metadata":{"id":"SYflANI7irC5"}},{"cell_type":"markdown","source":["###9.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"WvgKf0NEi8Tj"}},{"cell_type":"code","source":["\n","# =============================================================================\n","# CELL 9 — Model Card + Governance Memo (Structured, Constrained)\n","# =============================================================================\n","\n","# ---------- 1. Model Card (covers BOTH capsules) ----------\n","MODEL_CARD = {\n","    \"run_id\": RUN_ID,\n","    \"verification_status\": \"Not verified\",\n","\n","    # --- Scope ---\n","    \"intended_use\": [\n","        \"Teaching governance of graph-based models in high-accountability settings.\",\n","        \"Demonstrating how to produce auditable artifacts for relational inference.\",\n","        \"Generating exploratory hypotheses for human review — NOT for decisions.\"\n","    ],\n","    \"out_of_scope\": [\n","        \"Any real-world decision, ranking, or eligibility determination.\",\n","        \"Production deployment without full re-validation on real data.\",\n","        \"Causal inference from graph structure.\",\n","        \"Any use that does not include mandatory human review.\"\n","    ],\n","\n","    # --- Limitations ---\n","    \"limitations\": [\n","        \"All data is synthetic. No real-world signal is present.\",\n","        \"SVD embeddings capture only linear spectral structure; complex non-linear patterns are missed.\",\n","        \"The GCN is 2-layer (max 2 hops). Longer-range dependencies are invisible.\",\n","        \"Class imbalance (review_flag ≈ 8%) may inflate or deflate metrics. Secondary metrics should be interpreted cautiously.\",\n","        \"Sensitivity tests use only 3 perturbation types. Real-world stability may differ.\"\n","    ],\n","\n","    # --- Boundary & Edge Ambiguity ---\n","    \"boundary_summary\": {\n","        \"node_types_included\": [\"account\", \"vendor\", \"device\"],\n","        \"edge_types_included\": [\"transaction\", \"shared_device\", \"co_supply\"],\n","        \"time_window\": \"180 days (synthetic)\",\n","        \"exclusions\": \"No real data, no external sources, no PII.\"\n","    },\n","    \"edge_ambiguity_notes\": {\n","        \"shared_device\": \"Ambiguous proxy — excluded from GNN adjacency. If included in embeddings, \"\n","                         \"it introduces proxy-dominance risk. See risk_log.\",\n","        \"co_supply\": \"Similarity edge, not causal. Vendor co-occurrence does not imply shared intent.\",\n","        \"transaction\": \"Synthetic frequency. Does not represent real monetary value.\"\n","    },\n","\n","    # --- Propagation ---\n","    \"propagation_constraints_summary\": {\n","        \"max_hops\": cfg[\"gnn_max_hops\"],\n","        \"allowed_edge_types_in_gnn\": list(cfg[\"gnn_allowed_edge_types\"]),\n","        \"influence_clip\": cfg[\"gnn_influence_clip\"],\n","        \"normalisation\": \"symmetric D^{-1/2} A D^{-1/2}\"\n","    },\n","\n","    # --- Stability ---\n","    \"stability_summary\": {\n","        \"perturbations_tested\": [\"edge_dropout\", \"edge_noise\", \"time_window_shift\"],\n","        \"embeddings_passed\": emb_interpretation_allowed,\n","        \"gnn_passed\": gnn_interpretation_allowed\n","    },\n","\n","    # --- Oversight ---\n","    \"human_oversight\": {\n","        \"required\": True,\n","        \"scope\": \"All outputs require domain-expert review before any use.\",\n","        \"review_checklist_location\": \"memo/governance_memo.json → questions_to_verify\"\n","    },\n","\n","    # --- Prohibited Uses ---\n","    \"prohibited_uses\": [\n","        \"Automated ranking or scoring for action.\",\n","        \"Eligibility or adverse-action decisions.\",\n","        \"Causal claims from structural observations.\",\n","        \"Normative labelling without independent domain validation.\"\n","    ],\n","\n","    # --- Mandatory Statements ---\n","    \"mandatory_statements\": {\n","        \"structure_is_not_causality\": \"Structure is not causality.\",\n","        \"connectivity_is_not_explanation\": \"Connectivity is not explanation.\"\n","    }\n","}\n","stable_json_dump(PATHS[\"model_card\"], MODEL_CARD)\n","\n","# ---------- 2. Governance Memo (EXACTLY 7 top-level keys) ----------\n","# Gather stability outcome strings\n","emb_stab_outcomes = []\n","for pr in SENSITIVITY_DATA.get(\"embeddings\", []):\n","    status = \"PASS\" if (pr[\"mean_neighbor_overlap_at_k\"] >= cfg[\"neighbor_overlap_min\"] and\n","                        pr[\"mean_cosine_drift\"] <= cfg[\"score_drift_max\"]) else \"FAIL\"\n","    emb_stab_outcomes.append(f\"{pr['perturbation']}: overlap={pr['mean_neighbor_overlap_at_k']}, \"\n","                             f\"drift={pr['mean_cosine_drift']} → {status}\")\n","gnn_stab_outcomes = []\n","for pr in SENSITIVITY_DATA.get(\"gnn\", []):\n","    status = \"PASS\" if (pr[\"mean_score_drift\"] <= cfg[\"score_drift_max\"] and\n","                        pr[\"prediction_flip_rate\"] <= 0.20) else \"FAIL\"\n","    gnn_stab_outcomes.append(f\"{pr['perturbation']}: drift={pr['mean_score_drift']}, \"\n","                             f\"flip={pr['prediction_flip_rate']} → {status}\")\n","\n","GOVERNANCE_MEMO = {\n","    \"facts_provided\": {\n","        \"run_id\": RUN_ID,\n","        \"config_hash\": CONFIG_HASH,\n","        \"graph_stats\": summarize_graph_stats(G),\n","        \"validation_outcome\": \"passed — all schema rules satisfied\",\n","        \"stability_outcomes_embeddings\": emb_stab_outcomes,\n","        \"stability_outcomes_gnn\": gnn_stab_outcomes,\n","        \"decision_status\": overall_status,\n","        \"interpretation_allowed_embeddings\": emb_interpretation_allowed,\n","        \"interpretation_allowed_gnn\": gnn_interpretation_allowed,\n","        \"total_risk_events\": len(RISK_LOG_EVENTS),\n","        \"total_guardrail_events\": len(GUARDRAILS_EVENTS)\n","    },\n","    \"assumptions\": [\n","        \"Hypothesis (not verified): Nodes clustered together in embedding space MAY share latent behavioural similarity. \"\n","        \"This does NOT confirm shared intent, collusion, or any normative property.\",\n","        \"Hypothesis (not verified): The hub node's high centrality MAY reflect structural importance within the synthetic graph, \"\n","        \"but this is an artefact of the generation process and carries no real-world meaning.\",\n","        \"Hypothesis (not verified): The ambiguous connector's many shared_device edges MAY indicate a co-location pattern, \"\n","        \"but shared device access does NOT imply shared ownership or coordinated behaviour.\",\n","        \"Hypothesis (not verified): GNN predictions on the synthetic review_flag label MAY reflect learnable neighbourhood patterns, \"\n","        \"but these patterns are artifacts of the synthetic data generator.\",\n","        \"Assumption: The synthetic time window (180 days) is arbitrary. A different window would produce different graph structure.\"\n","    ],\n","    \"open_items\": [\n","        \"Data access approvals: No real data has been used. Before any real-world application, formal data-access and privacy review is required.\",\n","        \"Privacy review: PII handling, consent, and data-minimisation policies must be assessed.\",\n","        \"Bias testing: Systematic fairness testing across protected characteristics is required before any deployment.\",\n","        \"Domain sign-off: A subject-matter expert must validate edge semantics, boundary definitions, and feature provenance.\",\n","        \"Proxy feature review: synth_proxy_device_count is flagged as a proxy. Its inclusion requires explicit domain justification.\",\n","        \"Real-data re-validation: All schemas, thresholds, and stability tests must be re-run on real data before production use.\"\n","    ],\n","    \"analysis\": (\n","        \"This notebook demonstrates governance of two graph-based models on synthetic data. \"\n","        \"Capsule A (Graph Embeddings) converts adjacency structure into dense node representations via SVD, \"\n","        \"then tests stability under edge dropout, noise, and time-window shift. \"\n","        \"Capsule B (GNN) performs 2-hop message passing on a filtered adjacency (excluding ambiguous shared_device edges) \"\n","        \"and trains a node classifier on a synthetic label. Both capsules produce sensitivity reports that drive \"\n","        \"a deterministic abstention gate. The decision status reflects whether stability thresholds were met. \"\n","        \"All outputs are hypotheses requiring human verification. No causal or normative claims are made.\"\n","    ),\n","    \"draft_output\": (\n","        \"Draft output (Not verified): The notebook has completed end-to-end execution. \"\n","        f\"Decision status = '{overall_status}'. \"\n","        f\"Embeddings interpretation allowed = {emb_interpretation_allowed}. \"\n","        f\"GNN interpretation allowed = {gnn_interpretation_allowed}. \"\n","        \"All artifact files have been written to the artifact bundle. \"\n","        \"A human reviewer should examine questions_to_verify before any further use.\"\n","    ),\n","    \"verification_status\": \"Not verified\",\n","    \"questions_to_verify\": [\n","        \"Boundary validity: Are the included node and edge types appropriate for the intended use case? \"\n","        \"Should any types be added or removed?\",\n","        \"Edge semantics: Do the edge-type definitions (transaction, shared_device, co_supply) accurately reflect \"\n","        \"the intended real-world relationships? Are the ambiguity notes complete?\",\n","        \"Missingness: Is the 'missing = uninformative' assumption valid, or could missing edges carry signal?\",\n","        \"Propagation radius: Is 2-hop message passing appropriate, or should max_hops be adjusted based on domain knowledge?\",\n","        \"Sensitivity results: Are the observed stability outcomes acceptable for the intended use? \"\n","        \"Should thresholds be tightened or relaxed?\",\n","        \"Proxy risks: Is synth_proxy_device_count (or any real-world analogue) acceptable as a node feature? \"\n","        \"What domain evidence supports or refutes its inclusion?\",\n","        \"Centrality sanity: Has the hub node been accounted for in interpretation? \"\n","        \"Are there other high-degree nodes whose centrality is spurious?\",\n","        \"No decision authority: Confirm that no downstream system will use these outputs to make or influence \"\n","        \"automated decisions without explicit human review at each step.\"\n","    ]\n","}\n","stable_json_dump(PATHS[\"governance_memo\"], GOVERNANCE_MEMO)\n","\n","print(\"✅ Cell 9 complete — Model card and governance memo written.\")\n","print(f\"   Memo top-level keys: {list(GOVERNANCE_MEMO.keys())}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BkncoSvJnhk9","executionInfo":{"status":"ok","timestamp":1769956335161,"user_tz":360,"elapsed":40,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"56e851aa-d3b4-4037-ef1f-2660210d524c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Cell 9 complete — Model card and governance memo written.\n","   Memo top-level keys: ['facts_provided', 'assumptions', 'open_items', 'analysis', 'draft_output', 'verification_status', 'questions_to_verify']\n"]}]},{"cell_type":"markdown","source":["##10.AUDIT BUNDLE"],"metadata":{"id":"ss5L2x1ojA8d"}},{"cell_type":"markdown","source":["###10.1.OVERVIEW"],"metadata":{"id":"sduxTxx7jC0M"}},{"cell_type":"markdown","source":["**Cell 10 — Sample Outputs + Refusal Example + Zip Bundle + Summary**\n","\n","This cell is the end of execution, and it closes the notebook with three acts. The first is producing sample outputs that demonstrate what the notebook is willing to say and what it refuses to say. The second is packaging every artifact into a zip bundle. The third is printing a run summary that functions as a governance dashboard.\n","\n","The sample outputs include embedding neighbor hypotheses for four anchor nodes. Each hypothesis states which nodes appeared closest in embedding space, explicitly labels the observation as unverified, and explicitly states that proximity does not imply shared behavior, intent, or risk. A community structural summary describes the connected components of the graph: how many exist, how large the largest is. It labels this observation as structural description only and explicitly states that community membership does not imply coordinated activity or any normative property.\n","\n","The refusal example is the most pedagogically important element in this cell. A prohibited question is posed: \"Who should we investigate?\" The notebook refuses. The refusal text states that the notebook does not identify entities for investigation, ranking, or adverse action. It states that graph structure is observational only. It states that interpreting structure as actionable guidance violates the governance boundary. It closes with the two mandatory sentences: structure is not causality, connectivity is not explanation. The refusal is recorded as an enforcement event in the guardrails report. It is not an afterthought. It is a demonstration that the governance boundary is real and that it is enforced, not merely stated.\n","\n","The zip bundle packages all eighteen required artifact files into a single compressed archive. A verification loop then checks that every required file exists on disk, printing a checkmark or a failure marker beside each one. If any artifact is missing, the summary flags it immediately. The run summary prints the run identifier, the config hash, the overall decision status, the interpretation flags for both capsules, the human review requirement, and the full artifact checklist. It closes with a reminder that all outputs are unverified and that human review is mandatory. The notebook does not end with a result. It ends with an obligation. The artifacts are complete. The decision has been made by the gate. What happens next — whether these hypotheses are examined, whether a domain expert reviews the memo, whether any output reaches a production system — is entirely in human hands. That handoff is the point of the entire notebook."],"metadata":{"id":"XipzGKk72QXC"}},{"cell_type":"markdown","source":["###10.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"v_giRgoJjGY5"}},{"cell_type":"code","source":["\n","# =============================================================================\n","# CELL 10 — Sample Outputs + Refusal Example + Zip Bundle + Print Summary\n","# =============================================================================\n","import zipfile\n","\n","# ---------- 1. Build sample outputs ----------\n","# (a) Embedding neighbor hypotheses (a few anchors)\n","embedding_neighbor_hypotheses = []\n","for ai in anchor_indices[:4]:   # show 4 examples\n","    nid = all_node_ids_ordered[ai]\n","    ntype = G.nodes[nid].get(\"node_type\", \"unknown\")\n","    nbr_ids = [all_node_ids_ordered[ni] for ni in sorted(base_neighbors[ai])]\n","    nbr_types = [G.nodes[n].get(\"node_type\", \"unknown\") for n in nbr_ids]\n","    embedding_neighbor_hypotheses.append({\n","        \"anchor_node\": nid,\n","        \"anchor_type\": ntype,\n","        \"hypothesis\": f\"Nodes {nbr_ids} (types: {nbr_types}) appear closest in embedding space to {nid}. \"\n","                      \"This MAY indicate shared structural neighbourhood but is NOT verified and does NOT \"\n","                      \"imply shared behaviour, intent, or risk. This is a hypothesis only.\",\n","        \"verification_status\": \"Not verified\"\n","    })\n","\n","# (b) Community / structural summary (structural description only)\n","# Compute connected components on undirected version for illustration\n","G_undir = G.to_undirected()\n","components = list(nx.connected_components(G_undir))\n","components_sorted = sorted(components, key=len, reverse=True)\n","community_summary = {\n","    \"n_components\": len(components_sorted),\n","    \"largest_component_size\": len(components_sorted[0]) if components_sorted else 0,\n","    \"smallest_component_size\": len(components_sorted[-1]) if components_sorted else 0,\n","    \"structural_description\": (\n","        f\"The graph contains {len(components_sorted)} connected component(s). \"\n","        f\"The largest spans {len(components_sorted[0])} nodes. \"\n","        \"This is a structural observation only. Community membership does NOT imply shared behaviour, \"\n","        \"coordinated activity, or any normative property. This is a hypothesis. Not verified.\"\n","    ),\n","    \"verification_status\": \"Not verified\"\n","}\n","\n","# (c) Refusal example — user asks a prohibited question\n","PROHIBITED_QUESTION = \"Who should we investigate?\"\n","REFUSAL_TEXT = (\n","    \"REFUSED. This notebook does not identify entities for investigation, ranking, or any form of \"\n","    \"adverse action. Graph structure (centrality, community, similarity) is observational only. \"\n","    \"Interpreting structure as actionable guidance violates the governance boundary of this notebook. \"\n","    \"See guardrails_report.json for allowed and prohibited uses. \"\n","    \"All outputs are labeled 'Not verified' and require human domain review before any use. \"\n","    \"Structure is not causality. Connectivity is not explanation.\"\n",")\n","\n","# Record the refusal as an enforcement event\n","refusal_enforcement_event = {\n","    \"event_type\": \"boundary_refusal\",\n","    \"prohibited_query\": PROHIBITED_QUESTION,\n","    \"refusal_text\": REFUSAL_TEXT,\n","    \"timestamp\": now_utc_iso(),\n","    \"verification_status\": \"Not verified\"\n","}\n","GUARDRAILS_EVENTS.append(refusal_enforcement_event)\n","\n","# Update guardrails report with the refusal event\n","guardrails_data = json.load(open(PATHS[\"guardrails_report\"], \"r\"))\n","guardrails_data[\"enforcement_events\"].append(refusal_enforcement_event)\n","stable_json_dump(PATHS[\"guardrails_report\"], guardrails_data)\n","\n","# ---------- 2. Write sample_outputs.json ----------\n","# Include embedding matrix hash for traceability\n","emb_hash = sha256_dict({\"embedding_matrix_shape\": list(emb_base.shape),\n","                         \"embedding_matrix_checksum\": float(emb_base.sum())})\n","\n","SAMPLE_OUTPUTS = {\n","    \"run_id\": RUN_ID,\n","    \"verification_status\": \"Not verified\",\n","    \"embedding_neighbor_hypotheses\": embedding_neighbor_hypotheses,\n","    \"community_structural_summary\": community_summary,\n","    \"embedding_matrix_trace\": {\n","        \"shape\": list(emb_base.shape),\n","        \"hash\": emb_hash,\n","        \"note\": \"Full embedding matrix not serialised here for size. Hash provides integrity check.\"\n","    },\n","    \"refusal_example\": {\n","        \"user_query\": PROHIBITED_QUESTION,\n","        \"response\": REFUSAL_TEXT,\n","        \"enforcement_event_recorded\": True,\n","        \"verification_status\": \"Not verified\"\n","    },\n","    \"gnn_test_predictions_summary\": {\n","        \"n_test_accounts\": int(test_mask.sum().item()),\n","        \"predicted_0\": int((test_preds == 0).sum()),\n","        \"predicted_1\": int((test_preds == 1).sum()),\n","        \"note\": \"Predictions are on synthetic labels only. Not verified. Must not drive decisions.\",\n","        \"verification_status\": \"Not verified\"\n","    }\n","}\n","stable_json_dump(PATHS[\"sample_outputs\"], SAMPLE_OUTPUTS)\n","\n","# ---------- 3. Create artifacts_bundle.zip ----------\n","zip_path = PATHS[\"bundle_zip\"]\n","with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n","    for root, dirs, files in os.walk(ARTIFACT_ROOT):\n","        for fname in files:\n","            if fname == \"artifacts_bundle.zip\":\n","                continue   # don't include the zip inside itself\n","            fpath = os.path.join(root, fname)\n","            arcname = os.path.relpath(fpath, os.path.dirname(ARTIFACT_ROOT))\n","            zf.write(fpath, arcname)\n","\n","# ---------- 4. Verify all required artifact files exist ----------\n","REQUIRED_ARTIFACTS = [\n","    \"run_manifest.json\",\n","    os.path.join(\"schemas\", \"node_schema.json\"),\n","    os.path.join(\"schemas\", \"edge_schema.json\"),\n","    os.path.join(\"schemas\", \"graph_construction_manifest.json\"),\n","    os.path.join(\"validation\", \"data_validation_log.json\"),\n","    os.path.join(\"split\", \"split_manifest.json\"),\n","    os.path.join(\"metrics\", \"baseline_metrics.json\"),\n","    os.path.join(\"metrics\", \"train_metrics.json\"),\n","    os.path.join(\"metrics\", \"eval_metrics.json\"),\n","    os.path.join(\"reports\", \"propagation_constraints.json\"),\n","    os.path.join(\"reports\", \"sensitivity_report.json\"),\n","    os.path.join(\"reports\", \"guardrails_report.json\"),\n","    os.path.join(\"model\", \"model_card.json\"),\n","    os.path.join(\"decision\", \"decision.json\"),\n","    os.path.join(\"risk\", \"risk_log.json\"),\n","    os.path.join(\"memo\", \"governance_memo.json\"),\n","    os.path.join(\"outputs\", \"sample_outputs.json\"),\n","    \"artifacts_bundle.zip\"\n","]\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"  CHAPTER 4 — RUN SUMMARY\")\n","print(\"=\" * 70)\n","print(f\"  RUN_ID              : {RUN_ID}\")\n","print(f\"  CONFIG_HASH         : {CONFIG_HASH[:24]}…\")\n","print(f\"  OVERALL STATUS      : {overall_status}\")\n","print(f\"  Embeddings interp.  : {'ALLOWED' if emb_interpretation_allowed else 'BLOCKED'}\")\n","print(f\"  GNN interp.         : {'ALLOWED' if gnn_interpretation_allowed else 'BLOCKED'}\")\n","print(f\"  Human review req.   : True (always)\")\n","print(f\"  Verification status : Not verified\")\n","print(\"-\" * 70)\n","print(\"  ARTIFACT CHECK:\")\n","all_present = True\n","for req in REQUIRED_ARTIFACTS:\n","    full_path = os.path.join(ARTIFACT_ROOT, req)\n","    exists = os.path.isfile(full_path)\n","    status_icon = \"✅\" if exists else \"❌\"\n","    print(f\"    {status_icon}  {req}\")\n","    if not exists:\n","        all_present = False\n","print(\"-\" * 70)\n","print(f\"  All required artifacts present: {'YES ✅' if all_present else 'NO ❌ — CHECK ABOVE'}\")\n","print(f\"  Bundle zip           : {zip_path}\")\n","print(f\"  Bundle size          : {os.path.getsize(zip_path):,} bytes\")\n","print(\"=\" * 70)\n","print(\"\\n⚠️  All outputs are 'Not verified'. Human review is mandatory.\")\n","print(\"    Structure is not causality. Connectivity is not explanation.\")\n","print(\"=\" * 70)\n"],"metadata":{"id":"Lrqr5DdYjK83","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769956374503,"user_tz":360,"elapsed":199,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"ccb5b00c-8201-47dd-8634-30ffbc268126"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","  CHAPTER 4 — RUN SUMMARY\n","======================================================================\n","  RUN_ID              : run_20260201T142447_d6e413a4\n","  CONFIG_HASH         : 6e7c6b6060052e638098e3db…\n","  OVERALL STATUS      : abstain\n","  Embeddings interp.  : BLOCKED\n","  GNN interp.         : ALLOWED\n","  Human review req.   : True (always)\n","  Verification status : Not verified\n","----------------------------------------------------------------------\n","  ARTIFACT CHECK:\n","    ✅  run_manifest.json\n","    ✅  schemas/node_schema.json\n","    ✅  schemas/edge_schema.json\n","    ✅  schemas/graph_construction_manifest.json\n","    ✅  validation/data_validation_log.json\n","    ✅  split/split_manifest.json\n","    ✅  metrics/baseline_metrics.json\n","    ✅  metrics/train_metrics.json\n","    ✅  metrics/eval_metrics.json\n","    ✅  reports/propagation_constraints.json\n","    ✅  reports/sensitivity_report.json\n","    ✅  reports/guardrails_report.json\n","    ✅  model/model_card.json\n","    ✅  decision/decision.json\n","    ✅  risk/risk_log.json\n","    ✅  memo/governance_memo.json\n","    ✅  outputs/sample_outputs.json\n","    ✅  artifacts_bundle.zip\n","----------------------------------------------------------------------\n","  All required artifacts present: YES ✅\n","  Bundle zip           : ./artifacts/run_20260201T142447_d6e413a4/artifacts_bundle.zip\n","  Bundle size          : 822,643 bytes\n","======================================================================\n","\n","⚠️  All outputs are 'Not verified'. Human review is mandatory.\n","    Structure is not causality. Connectivity is not explanation.\n","======================================================================\n"]}]},{"cell_type":"markdown","source":["##11.CONCLUSION"],"metadata":{"id":"ZmL0B12KjLXl"}},{"cell_type":"markdown","source":["**Conclusion: What the Graph Taught Us About Governance**\n","\n","This notebook is finished. The artifacts are written, the bundle is zipped, the decision gate has spoken. But the notebook finishing is not the same as the work finishing. Everything produced here — every embedding, every prediction, every hypothesis in the sample outputs — is raw material. It becomes useful only when a human picks it up, reads the governance memo, and decides what to do next. This conclusion walks through what we actually built, what it revealed, how it differs from what came before, and where the responsibility now shifts to you.\n","\n","**How We Pursued the Three Objectives**\n","\n","The first objective was to make the conversion from adjacency to signal visible and auditable. We did that by choosing the simplest possible embedding method — truncated SVD on the adjacency matrix — and the simplest possible GNN — a two-layer graph convolutional network written by hand in PyTorch, no library abstractions. These choices were not limitations. They were governance decisions. When a method is simple enough to trace by hand, you can reason about exactly what it is doing to the data. You can point to the adjacency matrix, point to the SVD, and say: this is where the structure became a number. That traceability is what the artifact bundle captures. The train metrics record what went in. The embedding hash in the sample outputs confirms what came out. The gap between those two artifacts is where a human reviewer can ask: does this transformation make sense for the domain?\n","\n","The second objective was to surface relational risk in a way that could not be ignored. We did not describe propagation risk, boundary risk, and proxy risk in the abstract. We built them into the synthetic graph. The hub node exists so that you can see, in the PCA plot, what a high-degree node looks like in embedding space — and then ask whether its position there is meaningful or an artifact of degree. The connector node exists so that you can see what happens when ambiguous edges (shared devices) enter the embedding input — and then check whether the propagation constraints artifact correctly excluded them from the GNN. The time-window shift test exists so that you can see what happens when the boundary moves — and then decide whether the stability results are acceptable for whatever real-world use case you have in mind. Each risk was not just flagged. It was demonstrated, measured, and recorded.\n","\n","The third objective was to show that governance artifacts constrain interpretation rather than merely documenting it after the fact. The decision gate in Cell 8 is deterministic. It reads the sensitivity report, compares values to thresholds, and sets the overall status. No human touches that logic. If stability fails, interpretation is blocked. The system does not suggest caution. It does not add a disclaimer. It stops. The guardrails report records every enforcement event, including the refusal example in Cell 10, where the notebook was asked who should be investigated and declined to answer. That refusal is not an edge case. It is the boundary of the system made visible.\n","\n","**What This Chapter Added That Previous Chapters Did Not**\n","\n","Chapters 1 through 3 governed data that lived in tables. Each row was an independent observation. Bias in one row did not infect another. A bad feature in one sample did not propagate to its neighbors. Chapter 4 broke that assumption. In a graph, every node is a function of its neighborhood. That single change — from independence to interdependence — introduced three governance problems that did not exist before.\n","\n","The first is propagation. In Chapter 2, a proxy feature could mislead the model about a single prediction. In Chapter 4, a proxy edge can mislead the model about every node within two hops of where the proxy enters the graph. The GNN does not see individual edges. It sees aggregated neighborhood signals. A single ambiguous edge, repeated across a cluster, becomes a structural bias that the model cannot distinguish from a real pattern. The propagation constraints artifact is the answer: it records which edge types are permitted in message passing, how many hops are allowed, and why. Without that artifact, there is no way to audit whether the GNN's neighborhood was clean.\n","\n","The second is boundary. In Chapter 1, the data was a fixed table. The boundary was defined by which columns were included. In Chapter 4, the boundary is defined by which edges exist, which timestamps fall within the window, and which node types are in scope. Every one of those choices changes the graph the model sees. The graph construction manifest records those choices. The time-window shift test measures what happens when one of them changes. If the model's output shifts significantly when the boundary moves, the output is boundary-dependent — it is a function of when you looked, not just what you looked at. That is a different kind of instability than anything in Chapters 1 through 3, and it required a different kind of test.\n","\n","The third is proxy dominance through edge ambiguity. In Chapter 2, proxy features were detectable because they were columns in a table. You could inspect them, flag them, remove them. In Chapter 4, proxy edges are structural. A shared\\_device edge is not a column you can drop. It is a connection that participates in the topology of the graph. Removing it changes the graph. Keeping it introduces ambiguity into every embedding and every aggregation that touches it. The notebook handled this by excluding shared\\_device from the GNN adjacency entirely — a governance rule, not a modeling choice — while leaving it in the embedding input to demonstrate the risk. The edge schema documents the ambiguity. The risk log records the warning. The human reviewer must decide whether that exclusion is correct for their domain.\n","\n","**What the Results Actually Mean**\n","\n","The embedding PCA plot showed three clusters corresponding to the three node types. That is expected. The adjacency structure separates accounts from vendors from devices because they connect differently. The plot did not reveal hidden communities or suspicious subgroups. It revealed what the synthetic graph was designed to reveal: that structure becomes visible when projected into embedding space, and that visibility does not equal meaning.\n","\n","The GNN training loss decreased. The confusion matrix showed heavy prediction of the majority class. The test accuracy and F1 scores were modest. None of that is surprising. The review\\_flag label was assigned at random to eight percent of accounts. There is no real signal in the neighborhood structure for the GNN to find. The metrics are not evidence of model quality. They are evidence that the governance workflow ran correctly: the model trained, it produced outputs, and those outputs were measured against a held-out set.\n","\n","The stability tests produced the most consequential results. Whether the decision gate landed on \"abstain\" or \"pass\\_exploratory\" depends entirely on whether the sensitivity metrics breached their thresholds. If the status is \"abstain,\" interpretation is blocked — not because something failed catastrophically, but because the model's outputs shifted enough under perturbation that trusting them without human review would be irresponsible. If the status is \"pass\\_exploratory,\" interpretation is permitted — but only as hypothesis generation, and only after a human has reviewed the questions listed in the governance memo.\n","\n","**Where the Responsibility Now Shifts to You**\n","\n","The notebook executed every check it was designed to execute. It validated the schema. It measured stability. It enforced guardrails. It wrote eighteen artifact files. It refused to answer a prohibited question. All of that is automated. All of that is deterministic. None of it requires human judgment.\n","\n","What requires human judgment is everything that comes after. The governance memo lists the open items explicitly. Data access approvals have not been obtained — this notebook used synthetic data. Privacy review has not been performed. Bias testing across protected characteristics has not been conducted. Domain sign-off on edge semantics has not happened. The proxy feature (synth\\_proxy\\_device\\_count) has been flagged but not independently evaluated. The questions\\_to\\_verify section of the memo is a checklist. Every item on it must be resolved by a person before any output from this notebook moves closer to production.\n","\n","The model card states two sentences that govern everything this notebook produced: **structure is not causality**, and **connectivity is not explanation**. No graph observation — no cluster, no centrality score, no embedding proximity — can be used as evidence that something is true about the real world. It can be used as a hypothesis. It can be used to decide where a human expert should look next. It cannot be used to decide what to do. The notebook does not cross that boundary. It hands you the artifacts, points to the memo, and waits.\n","\n","**Capability ↑ ⇒ Risk ↑ ⇒ Controls must ↑.** Chapter 4 added relational capability. It added relational risk. It added relational controls. The pattern holds. It will hold in Chapter 5 as well. The models will get more capable. The risks will get more subtle. The controls will get more demanding. And at every stage, the notebook will do what it was built to do: produce the evidence, enforce the boundaries, and hand the decisions to you."],"metadata":{"id":"hQ7vw0WXzUKp"}}]}