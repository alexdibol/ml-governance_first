# Governed Machine Learning (Governance-First)
## A Foundation Before Generative AI

## What this repository contains

This repository supports the book  
**Governed Machine Learning (Governance-First)** —  
the foundational volume that precedes (and makes possible) the rest of the
**Governance-First AI** collection.

This is not a “machine learning tricks” repository.

It is a governance-first tour de force designed to build the institutional
discipline required to use any form of AI responsibly — especially in
high-accountability environments where mistakes are not “bugs,” but
operational, regulatory, and reputational events.

Machine learning is where most organizational AI risk actually begins:
data leakage, unstable assumptions, silent drift, false confidence,
and decision laundering disguised as “analytics.”

This book treats ML as it should be treated:
a capability that must be governed before it is trusted.

Every output is explicitly labeled: **“Not verified.”**


## Contents

### Book (PDF)

- **Book file (PDF):**  
  https://github.com/alexdibol/ml-governance_first/blob/main/book/GOVERNED%20MACHINE%20LEARNING.pdf


### Companion notebooks (Colab-ready)

- **Notebooks folder:**  
  https://github.com/alexdibol/ml-governance_first/tree/main/notebooks


### Direct Colab links

- **Chapter 1 — Unsupervised Learning: Pattern Without Intention**  
  https://colab.research.google.com/github/alexdibol/ml-governance_first/blob/main/notebooks/CHAPTER_1.ipynb

- **Chapter 2 — Single Neural Networks: Nonlinearity With Accountability**  
  https://colab.research.google.com/github/alexdibol/ml-governance_first/blob/main/notebooks/CHAPTER_2.ipynb

- **Chapter 3 — Multi-Model Systems: Autoencoders and GANs Without Illusions**  
  https://colab.research.google.com/github/alexdibol/ml-governance_first/blob/main/notebooks/CHAPTER_3.ipynb

- **Chapter 4 — Graph Models: Relational Power, Relational Risk**  
  https://colab.research.google.com/github/alexdibol/ml-governance_first/blob/main/notebooks/CHAPTER_4.ipynb

- **Chapter 5 — Evolutionary and Optimization Systems: Search Under Control**  
  https://colab.research.google.com/github/alexdibol/ml-governance_first/blob/main/notebooks/CHAPTER_5.ipynb


## Who this is for

This book and repository are designed for:

• MBA and Master of Finance students  
• business practitioners in high-accountability environments  
• risk, compliance, and model governance teams  
• transformation and innovation leaders  
• instructors teaching applied AI responsibly  

No deep ML engineering background is required.

The objective is not performance.
The objective is **governance literacy**:
how to produce machine learning work that is auditable, reproducible,
and institutionally defensible.


## Core thesis: governance must precede capability

This repository enforces a non-negotiable premise:

**Capability ↑ ⇒ Risk ↑ ⇒ Controls ↑**

Machine learning often looks harmless because it is “just statistics.”
That is precisely why it is dangerous in organizations:
it produces outputs that look like facts and get operationalized without
sufficient evidence discipline.

This book trains a culture:

• separate pattern discovery from institutional meaning  
• treat datasets as risk-bearing assets  
• treat models as changeable artifacts, not truths  
• refuse decision authority by design  
• document the difference between facts, assumptions, and unknowns  
• log what happened, why it happened, and who approved it  


## Companion notebooks: strict governed format

The notebooks are not “tutorials.”  
They are governance demonstrators.

Each chapter notebook implements **two governed ML capsules** (two models),
and each run produces a standardized artifact bundle that supports auditability
and reproducibility.

Non-negotiables across the notebooks:

• **synthetic data only** (no external pulls, no client data, no proprietary data)  
• **no autonomous decision authority** (no eligibility rules, no ranking people, no recommendations)  
• every narrative output labeled **“Not verified.”**  
• **explicit human accountability** and required review/sign-off framing  
• reproducible runs with deterministic controls where feasible  
• evidence-first evaluation (discipline > performance)  


## Standardized artifact bundle (every run)

Each execution generates a governed evidence package, including:

• run_manifest (run_id, config hash, environment fingerprint)  
• schemas (inputs/outputs + artifact schemas)  
• validation logs (data checks, constraint checks, guardrails checks)  
• split manifest (how data was partitioned and why)  
• metrics (reported with context, limitations, and “Not verified”)  
• model card (scope, intended use, prohibited use, limitations)  
• guardrails report (boundary tests, refusal behavior, unsafe patterns)  
• decision (explicit: no decision authority; outputs are informational only)  
• risk log (identified risks, mitigations, open items)  
• governance memo (ownership, approvals, accountability statements)  


## What makes this governance-first

Across all chapters, the framework enforces:

• explicit task scope and institutional boundaries  
• separation of generation vs verification vs approval  
• refusal and boundary-violation testing  
• uncertainty labeling and evidence discipline  
• reproducibility requirements (not “it worked once”)  
• audit-ready documentation as a first-class output  
• human review as an explicit control, not a footnote  

This is the foundation that prevents “AI theater”:
polished outputs that are not defensible.


## Important note

This repository and book are provided **for educational and research purposes only**.

They do **not** constitute:

• investment advice  
• legal advice  
• tax advice  
• compliance determinations  
• operational recommendations  

Human professional review is mandatory for any reliance-bearing use.

Confidential, proprietary, or privileged information must never be provided to
external systems. Use redaction and minimum-necessary inputs by default.


## Use of generative AI tools (transparency statement)

Generative AI tools may have been used to **assist** in drafting, editing,
formatting, or code scaffolding during development.

However:

• conceptual design  
• pedagogical structure  
• governance framework and controls  
• risk taxonomy and boundary definitions  
• artifact standards and evidence requirements  
• final editorial judgment and acceptance  

were **human-led, human-supervised, and human-approved** at all times.

The author assumes **full responsibility** for the content, structure,
interpretation, and conclusions presented in this work.


## Position within the Governance-First AI series

This book is the foundational volume that precedes the domain volumes:

• Governance-First AI for Accounting  
• Governance-First AI for Law  
• Governance-First AI for Financial Advice  
• Governance-First AI for Consulting  
• Governance-First AI for Investment Banking  
• Governance-First Fine-Tuning  

This volume exists to ensure the rest of the collection has a shared,
institutionally rigorous base layer:
**governed machine learning as a discipline, not a demo.**


## License

Released under the **MIT License**.


## Suggested citation

Alejandro Reynoso  
*Governed Machine Learning (Governance-First): A Foundation Before Generative AI*  
Companion repository and notebooks, GitHub.
